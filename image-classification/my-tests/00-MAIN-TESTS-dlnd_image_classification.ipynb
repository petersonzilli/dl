{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 5:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1014, 1: 1014, 2: 952, 3: 1016, 4: 997, 5: 1025, 6: 980, 7: 977, 8: 1003, 9: 1022}\n",
      "First 20 Labels: [1, 8, 5, 1, 5, 7, 4, 3, 8, 2, 7, 2, 0, 1, 5, 9, 6, 2, 0, 8]\n",
      "\n",
      "Example of Image 10:\n",
      "Image - Min Value: 16 Max Value: 227\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 7 Name: horse\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8K9WR44AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29a176ed4a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 5\n",
    "sample_id = 10\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    a = 0.0\n",
    "    b = 1.0\n",
    "    min_x = 0\n",
    "    max_x = 255\n",
    "    return a + ((x - min_x) * (b - a) / (max_x - min_x))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_item(cls):\n",
    "    number_of_classes = 10\n",
    "    ohi = np.zeros(number_of_classes)\n",
    "    ohi[cls] = 1\n",
    "    return ohi\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    result = np.array([one_hot_item(item) for item in x])\n",
    "    return result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    image_height, image_width, color_channels = image_shape\n",
    "    shape = [None, image_height, image_width, color_channels]\n",
    "    x = tf.placeholder(tf.float32, shape=shape, name='x')\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    shape = [None, n_classes]\n",
    "    y = tf.placeholder(tf.float32, shape=shape, name='y')\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d(x, weights, b, conv_strides = (1, 1)):\n",
    "    \n",
    "    #2. Apply a convolution to x_tensor using weight and conv_strides.\n",
    "    #      We recommend you use same padding, but you're welcome to use any padding.\n",
    "    #3. Add bias\n",
    "    #4. Add a nonlinear activation to the convolution.\n",
    "           \n",
    "    conv_stride_height, conv_stride_width = conv_strides\n",
    "    x = tf.nn.conv2d(x, weights, strides=[1, conv_stride_height, conv_stride_width, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    x = tf.nn.relu(x)\n",
    "    return tf.nn.lrn(x, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "def maxpool2d(x, pool_ksize=(2, 2), pool_strides = (2, 2)):\n",
    "           \n",
    "    #5. Apply Max Pooling using pool_ksize and pool_strides.\n",
    "    #      We recommend you use same padding, but you're welcome to use any padding.\n",
    "    \n",
    "    pool_kernel_size_height, pool_kernel_size_width = pool_ksize\n",
    "    pool_stride_height, pool_stride_width = pool_strides\n",
    "           \n",
    "    return tf.nn.max_pool(\n",
    "        x,\n",
    "        ksize=[1, pool_kernel_size_height, pool_kernel_size_width, 1],\n",
    "        strides=[1, pool_stride_height, pool_stride_width, 1],\n",
    "        padding='SAME')\n",
    "\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "\n",
    "    # Variable renaming...\n",
    "    input = x_tensor\n",
    "    input_shape = input.get_shape().as_list()\n",
    "    color_channels = input_shape[3]\n",
    "    filter_size_height, filter_size_width = conv_ksize\n",
    "    k_output = conv_num_outputs\n",
    "        \n",
    "    # 1. Create the weight and bias using conv_ksize, conv_num_outputs and the shape of x_tensor\n",
    "    weights = tf.Variable(tf.truncated_normal([filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    #bias = tf.Variable(tf.random_uniform(conv_num_outputs, -0.5, 0.5))\n",
    "    \n",
    "    x = conv2d(input, weights, bias, conv_strides)\n",
    "    x = maxpool2d(x, pool_ksize, pool_strides)\n",
    " \n",
    "    return x\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    flattened_image_size = np.prod(np.array(x_tensor.get_shape().as_list()[1:]))\n",
    "    flattened_x_tensor = tf.reshape(x_tensor, [-1, flattened_image_size])    \n",
    "    return flattened_x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "\n",
    "    # Variable renaming\n",
    "    input = x_tensor\n",
    "    input_shape = input.get_shape().as_list()\n",
    "    \n",
    "    # weights and biases...\n",
    "    weights_fc = tf.Variable(tf.truncated_normal([input_shape[1], num_outputs]))\n",
    "    #biases_fc = tf.Variable(tf.truncated_normal([num_outputs]))\n",
    "    biases_fc = tf.Variable(tf.zeros([num_outputs]))\n",
    "\n",
    "    fc = tf.add(tf.matmul(input, weights_fc), biases_fc)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    \n",
    "    return fc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    \n",
    "    # Variable renaming\n",
    "    input = x_tensor\n",
    "    input_shape = input.get_shape().as_list()\n",
    "    n_classes = num_outputs\n",
    "    \n",
    "    # weights and biases...\n",
    "    weights_out = tf.Variable(tf.random_normal([input_shape[1], n_classes]))\n",
    "    biases_out = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "    # Output Layer - class prediction\n",
    "    out = tf.add(tf.matmul(input, weights_out), biases_out)\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 128)\n",
      "(?, 4, 4, 512)\n",
      "(?, 8192)\n",
      "(?, 40960) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 128)\n",
      "(?, 4, 4, 512)\n",
      "(?, 8192)\n",
      "(?, 40960) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32 , 'cvmp2': 128, 'cvmp3': 512}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (5, 5)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 4096*10, 'fc2' : 4096*4, 'fc3' : 4096, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    #x = fully_conn(x, num_outputs['fc2'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # DONE: Implement Function\n",
    "    # Calculate batch loss and accuracy\n",
    "    train_loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    train_acc = sess.run(accuracy, feed_dict={ x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={ x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "\n",
    "    print('Training Loss: {:>10.4f} Trainning Accuracy: {:.6f} Validation Accuracy: {:.6f}'.format(\n",
    "        train_loss,\n",
    "        train_acc,\n",
    "        valid_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .2 # 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-07 01:09:04 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 9876278.0000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.350400\n",
      "2017-06-07 01:24:20 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 5764467.0000 Trainning Accuracy: 0.425000 Validation Accuracy: 0.400000\n",
      "2017-06-07 01:40:08 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 3412669.5000 Trainning Accuracy: 0.525000 Validation Accuracy: 0.420600\n",
      "2017-06-07 01:53:57 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 1680414.3750 Trainning Accuracy: 0.600000 Validation Accuracy: 0.455600\n",
      "2017-06-07 02:08:08 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 1055087.8750 Trainning Accuracy: 0.725000 Validation Accuracy: 0.464000\n",
      "2017-06-07 02:22:16 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 650779.8750 Trainning Accuracy: 0.800000 Validation Accuracy: 0.472200\n",
      "2017-06-07 02:36:32 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 512370.0000 Trainning Accuracy: 0.825000 Validation Accuracy: 0.475400\n",
      "2017-06-07 02:50:16 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 320519.5625 Trainning Accuracy: 0.850000 Validation Accuracy: 0.470200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1a4379a96ed4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-81050f085b79>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# DONE: Implement Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \"\"\"\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outros testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 128)\n",
      "(?, 4, 4, 512)\n",
      "(?, 8192)\n",
      "(?, 16384) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 128)\n",
      "(?, 4, 4, 512)\n",
      "(?, 8192)\n",
      "(?, 16384) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32 , 'cvmp2': 128, 'cvmp3': 512}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (5, 5)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 4096*4, 'fc2' : 4096*2, 'fc3' : 4096, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    #x = fully_conn(x, num_outputs['fc2'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .4 # 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-07 22:56:16 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 6603350.5000 Trainning Accuracy: 0.250000 Validation Accuracy: 0.312200\n",
      "2017-06-07 23:07:08 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 2334737.7500 Trainning Accuracy: 0.375000 Validation Accuracy: 0.381200\n",
      "2017-06-07 23:17:36 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 1054300.2500 Trainning Accuracy: 0.525000 Validation Accuracy: 0.432400\n",
      "2017-06-07 23:27:03 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 566304.6250 Trainning Accuracy: 0.625000 Validation Accuracy: 0.460800\n",
      "2017-06-07 23:36:24 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 360649.8125 Trainning Accuracy: 0.775000 Validation Accuracy: 0.456400\n",
      "2017-06-07 23:45:51 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 128501.9766 Trainning Accuracy: 0.875000 Validation Accuracy: 0.481400\n",
      "2017-06-07 23:54:16 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 97196.1094 Trainning Accuracy: 0.925000 Validation Accuracy: 0.484200\n",
      "2017-06-08 00:02:17 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 65658.6250 Trainning Accuracy: 0.900000 Validation Accuracy: 0.489200\n",
      "2017-06-08 00:10:12 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 16932.3320 Trainning Accuracy: 0.950000 Validation Accuracy: 0.490800\n",
      "2017-06-08 00:18:21 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:  4746.7251 Trainning Accuracy: 0.975000 Validation Accuracy: 0.497400\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-08 00:26:43 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 4391025.5000 Trainning Accuracy: 0.325000 Validation Accuracy: 0.334000\n",
      "2017-06-08 00:34:57 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 1660976.6250 Trainning Accuracy: 0.600000 Validation Accuracy: 0.406000\n",
      "2017-06-08 00:43:11 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 824451.5625 Trainning Accuracy: 0.675000 Validation Accuracy: 0.436400\n",
      "2017-06-08 00:51:19 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 455781.0625 Trainning Accuracy: 0.725000 Validation Accuracy: 0.448400\n",
      "2017-06-08 00:59:47 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 276156.8750 Trainning Accuracy: 0.750000 Validation Accuracy: 0.458400\n",
      "2017-06-08 01:07:57 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 152027.9688 Trainning Accuracy: 0.850000 Validation Accuracy: 0.475400\n",
      "2017-06-08 01:16:16 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 103479.6250 Trainning Accuracy: 0.925000 Validation Accuracy: 0.471600\n",
      "2017-06-08 01:24:11 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 77034.6719 Trainning Accuracy: 0.900000 Validation Accuracy: 0.475400\n",
      "2017-06-08 01:31:56 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 56686.1758 Trainning Accuracy: 0.975000 Validation Accuracy: 0.486200\n",
      "2017-06-08 01:39:41 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 47399.9883 Trainning Accuracy: 0.950000 Validation Accuracy: 0.487200\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .35 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# este foi o último que eu vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-08 01:47:32 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 7045248.0000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.335400\n",
      "2017-06-08 01:55:16 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 2751446.7500 Trainning Accuracy: 0.400000 Validation Accuracy: 0.387600\n",
      "2017-06-08 02:02:59 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 1622252.7500 Trainning Accuracy: 0.475000 Validation Accuracy: 0.417600\n",
      "2017-06-08 02:10:41 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 851781.8125 Trainning Accuracy: 0.650000 Validation Accuracy: 0.451800\n",
      "2017-06-08 02:18:22 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 494769.0938 Trainning Accuracy: 0.700000 Validation Accuracy: 0.458600\n",
      "2017-06-08 02:26:08 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 309180.1875 Trainning Accuracy: 0.725000 Validation Accuracy: 0.474400\n",
      "2017-06-08 02:33:55 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 204949.4375 Trainning Accuracy: 0.775000 Validation Accuracy: 0.466600\n",
      "2017-06-08 02:41:40 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 112342.1719 Trainning Accuracy: 0.825000 Validation Accuracy: 0.489400\n",
      "2017-06-08 02:49:23 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 119177.6016 Trainning Accuracy: 0.800000 Validation Accuracy: 0.489600\n",
      "2017-06-08 02:57:06 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 75237.3047 Trainning Accuracy: 0.850000 Validation Accuracy: 0.491400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .3 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-08 03:05:01 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 5553248.0000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.347200\n",
      "2017-06-08 03:12:44 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 2263325.7500 Trainning Accuracy: 0.500000 Validation Accuracy: 0.390800\n",
      "2017-06-08 03:20:27 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 1177011.5000 Trainning Accuracy: 0.650000 Validation Accuracy: 0.436000\n",
      "2017-06-08 03:28:11 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 769938.2500 Trainning Accuracy: 0.700000 Validation Accuracy: 0.460000\n",
      "2017-06-08 03:35:55 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 430351.6562 Trainning Accuracy: 0.775000 Validation Accuracy: 0.466800\n",
      "2017-06-08 03:43:38 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 271903.5938 Trainning Accuracy: 0.800000 Validation Accuracy: 0.465800\n",
      "2017-06-08 03:51:23 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 182492.2188 Trainning Accuracy: 0.800000 Validation Accuracy: 0.474600\n",
      "2017-06-08 03:59:06 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 128522.8750 Trainning Accuracy: 0.850000 Validation Accuracy: 0.483200\n",
      "2017-06-08 04:06:49 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 96992.3516 Trainning Accuracy: 0.900000 Validation Accuracy: 0.487800\n",
      "2017-06-08 04:14:30 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 82924.4219 Trainning Accuracy: 0.875000 Validation Accuracy: 0.487800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .25 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-08 04:22:20 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 4137614.0000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.339400\n",
      "2017-06-08 04:30:08 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 1302147.3750 Trainning Accuracy: 0.550000 Validation Accuracy: 0.410000\n",
      "2017-06-08 04:37:58 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 695381.0625 Trainning Accuracy: 0.650000 Validation Accuracy: 0.418600\n",
      "2017-06-08 04:45:45 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 366531.9375 Trainning Accuracy: 0.725000 Validation Accuracy: 0.435800\n",
      "2017-06-08 04:53:30 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 76880.3906 Trainning Accuracy: 0.875000 Validation Accuracy: 0.460000\n",
      "2017-06-08 05:01:12 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 24788.1387 Trainning Accuracy: 0.925000 Validation Accuracy: 0.467400\n",
      "2017-06-08 05:08:56 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 26239.3320 Trainning Accuracy: 0.950000 Validation Accuracy: 0.470600\n",
      "2017-06-08 05:16:40 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 10108.1904 Trainning Accuracy: 0.975000 Validation Accuracy: 0.471000\n",
      "2017-06-08 05:24:24 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.485200\n",
      "2017-06-08 05:32:09 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.483000\n",
      "2017-06-08 05:39:52 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.496400\n",
      "2017-06-08 05:47:36 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.493400\n",
      "2017-06-08 05:55:19 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.494200\n",
      "2017-06-08 06:03:02 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.499000\n",
      "2017-06-08 06:10:46 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.492200\n",
      "2017-06-08 06:18:29 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.506400\n",
      "2017-06-08 06:26:13 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.503600\n",
      "2017-06-08 06:33:56 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.509400\n",
      "2017-06-08 06:41:42 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.514200\n",
      "2017-06-08 06:49:26 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.503400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .45 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-08 08:11:57 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 6056817.5000 Trainning Accuracy: 0.375000 Validation Accuracy: 0.327000\n",
      "2017-06-08 08:19:39 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 2428278.7500 Trainning Accuracy: 0.500000 Validation Accuracy: 0.358800\n",
      "2017-06-08 08:27:31 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 1149792.8750 Trainning Accuracy: 0.625000 Validation Accuracy: 0.418600\n",
      "2017-06-08 08:35:15 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 628260.3750 Trainning Accuracy: 0.725000 Validation Accuracy: 0.439200\n",
      "2017-06-08 08:42:58 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 373709.5312 Trainning Accuracy: 0.800000 Validation Accuracy: 0.449600\n",
      "2017-06-08 08:50:40 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 169776.1562 Trainning Accuracy: 0.850000 Validation Accuracy: 0.463400\n",
      "2017-06-08 08:58:22 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 116186.9766 Trainning Accuracy: 0.900000 Validation Accuracy: 0.469200\n",
      "2017-06-08 09:06:03 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 58855.1133 Trainning Accuracy: 0.925000 Validation Accuracy: 0.482200\n",
      "2017-06-08 09:13:44 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 33851.5117 Trainning Accuracy: 0.925000 Validation Accuracy: 0.483800\n",
      "2017-06-08 09:21:26 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 30543.1602 Trainning Accuracy: 0.950000 Validation Accuracy: 0.488400\n",
      "2017-06-08 09:29:08 - Epoch 11, CIFAR-10 Batch 1:  Training Loss: 26019.1250 Trainning Accuracy: 0.975000 Validation Accuracy: 0.500000\n",
      "2017-06-08 09:36:51 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:  8866.9824 Trainning Accuracy: 0.975000 Validation Accuracy: 0.493200\n",
      "2017-06-08 09:44:31 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.495800\n",
      "2017-06-08 09:52:13 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.504400\n",
      "2017-06-08 09:59:53 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.505000\n",
      "2017-06-08 10:07:38 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.509200\n",
      "2017-06-08 10:15:22 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.508600\n",
      "2017-06-08 10:23:04 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.508400\n",
      "2017-06-08 10:30:47 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.511200\n",
      "2017-06-08 10:38:28 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.517800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .33 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXXXXXXXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 12)\n",
      "(?, 8, 8, 48)\n",
      "(?, 3072)\n",
      "(?, 6144) with dropout\n",
      "(?, 6144) with dropout\n",
      "(?, 1000) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 12)\n",
      "(?, 8, 8, 48)\n",
      "(?, 3072)\n",
      "(?, 6144) with dropout\n",
      "(?, 6144) with dropout\n",
      "(?, 1000) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 12 , 'cvmp2': 48, 'cvmp3': 512}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (5, 5)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 3072*2, 'fc2' : 3072*2, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    #x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    #print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    x = fully_conn(x, num_outputs['fc3'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 01:47:03 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 36301940.0000 Trainning Accuracy: 0.075000 Validation Accuracy: 0.105000\n",
      "2017-06-10 01:49:12 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 13628992.0000 Trainning Accuracy: 0.125000 Validation Accuracy: 0.118200\n",
      "2017-06-10 01:51:16 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 6927509.0000 Trainning Accuracy: 0.075000 Validation Accuracy: 0.131600\n",
      "2017-06-10 01:53:21 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 3899760.0000 Trainning Accuracy: 0.125000 Validation Accuracy: 0.149400\n",
      "2017-06-10 01:55:27 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 2688986.7500 Trainning Accuracy: 0.125000 Validation Accuracy: 0.164400\n",
      "2017-06-10 01:57:32 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 2241826.0000 Trainning Accuracy: 0.150000 Validation Accuracy: 0.178800\n",
      "2017-06-10 01:59:36 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 1558900.5000 Trainning Accuracy: 0.175000 Validation Accuracy: 0.189000\n",
      "2017-06-10 02:01:41 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 1199288.3750 Trainning Accuracy: 0.175000 Validation Accuracy: 0.193400\n",
      "2017-06-10 02:03:45 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 890085.3125 Trainning Accuracy: 0.225000 Validation Accuracy: 0.186800\n",
      "2017-06-10 02:05:49 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 703618.1250 Trainning Accuracy: 0.225000 Validation Accuracy: 0.187200\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .2 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 02:07:57 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 35970044.0000 Trainning Accuracy: 0.050000 Validation Accuracy: 0.164000\n",
      "2017-06-10 02:10:04 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 12032950.0000 Trainning Accuracy: 0.075000 Validation Accuracy: 0.208200\n",
      "2017-06-10 02:12:09 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 7272341.0000 Trainning Accuracy: 0.125000 Validation Accuracy: 0.229000\n",
      "2017-06-10 02:14:13 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 4632536.5000 Trainning Accuracy: 0.150000 Validation Accuracy: 0.232000\n",
      "2017-06-10 02:16:17 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 3280463.5000 Trainning Accuracy: 0.175000 Validation Accuracy: 0.233600\n",
      "2017-06-10 02:18:21 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 2463222.7500 Trainning Accuracy: 0.225000 Validation Accuracy: 0.240200\n",
      "2017-06-10 02:20:26 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 1977080.7500 Trainning Accuracy: 0.250000 Validation Accuracy: 0.238000\n",
      "2017-06-10 02:22:30 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 1632834.6250 Trainning Accuracy: 0.200000 Validation Accuracy: 0.240000\n",
      "2017-06-10 02:24:34 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 1314341.5000 Trainning Accuracy: 0.200000 Validation Accuracy: 0.242800\n",
      "2017-06-10 02:26:38 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 1048510.5000 Trainning Accuracy: 0.200000 Validation Accuracy: 0.240400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .25 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 02:28:45 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 22347928.0000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.167600\n",
      "2017-06-10 02:30:49 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 9840000.0000 Trainning Accuracy: 0.150000 Validation Accuracy: 0.203200\n",
      "2017-06-10 02:32:53 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 5324785.0000 Trainning Accuracy: 0.250000 Validation Accuracy: 0.221800\n",
      "2017-06-10 02:34:57 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 3895730.5000 Trainning Accuracy: 0.200000 Validation Accuracy: 0.226000\n",
      "2017-06-10 02:37:02 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 2614830.2500 Trainning Accuracy: 0.225000 Validation Accuracy: 0.229400\n",
      "2017-06-10 02:39:06 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 1910302.7500 Trainning Accuracy: 0.250000 Validation Accuracy: 0.227800\n",
      "2017-06-10 02:41:11 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 1350715.6250 Trainning Accuracy: 0.300000 Validation Accuracy: 0.231600\n",
      "2017-06-10 02:43:14 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 953403.0000 Trainning Accuracy: 0.350000 Validation Accuracy: 0.229000\n",
      "2017-06-10 02:45:18 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 641062.3750 Trainning Accuracy: 0.350000 Validation Accuracy: 0.220400\n",
      "2017-06-10 02:47:22 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 501712.5938 Trainning Accuracy: 0.275000 Validation Accuracy: 0.209800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .3 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 02:49:30 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 25322080.0000 Trainning Accuracy: 0.150000 Validation Accuracy: 0.138600\n",
      "2017-06-10 02:51:34 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 9503542.0000 Trainning Accuracy: 0.150000 Validation Accuracy: 0.164200\n",
      "2017-06-10 02:53:39 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 4110595.2500 Trainning Accuracy: 0.200000 Validation Accuracy: 0.204600\n",
      "2017-06-10 02:55:43 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 2829595.7500 Trainning Accuracy: 0.250000 Validation Accuracy: 0.219800\n",
      "2017-06-10 02:57:46 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 2249486.0000 Trainning Accuracy: 0.250000 Validation Accuracy: 0.228200\n",
      "2017-06-10 02:59:51 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 1901134.7500 Trainning Accuracy: 0.325000 Validation Accuracy: 0.228200\n",
      "2017-06-10 03:01:55 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 1431627.2500 Trainning Accuracy: 0.300000 Validation Accuracy: 0.233400\n",
      "2017-06-10 03:03:59 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 1027687.6250 Trainning Accuracy: 0.325000 Validation Accuracy: 0.237000\n",
      "2017-06-10 03:06:03 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 840560.0000 Trainning Accuracy: 0.300000 Validation Accuracy: 0.239000\n",
      "2017-06-10 03:08:07 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 753200.8750 Trainning Accuracy: 0.250000 Validation Accuracy: 0.234200\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .33 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 03:10:14 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 26760092.0000 Trainning Accuracy: 0.125000 Validation Accuracy: 0.156600\n",
      "2017-06-10 03:12:17 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 10044114.0000 Trainning Accuracy: 0.075000 Validation Accuracy: 0.166200\n",
      "2017-06-10 03:14:21 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 4257568.0000 Trainning Accuracy: 0.125000 Validation Accuracy: 0.198400\n",
      "2017-06-10 03:16:25 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 2420304.5000 Trainning Accuracy: 0.100000 Validation Accuracy: 0.206400\n",
      "2017-06-10 03:18:28 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 1455561.3750 Trainning Accuracy: 0.250000 Validation Accuracy: 0.218200\n",
      "2017-06-10 03:20:32 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 1036431.8125 Trainning Accuracy: 0.250000 Validation Accuracy: 0.229600\n",
      "2017-06-10 03:22:35 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 880669.6250 Trainning Accuracy: 0.300000 Validation Accuracy: 0.216600\n",
      "2017-06-10 03:24:39 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 680146.3125 Trainning Accuracy: 0.325000 Validation Accuracy: 0.204400\n",
      "2017-06-10 03:26:42 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 523227.3125 Trainning Accuracy: 0.325000 Validation Accuracy: 0.203600\n",
      "2017-06-10 03:28:46 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 411082.0938 Trainning Accuracy: 0.300000 Validation Accuracy: 0.198400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .4 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 03:30:53 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 27000436.0000 Trainning Accuracy: 0.100000 Validation Accuracy: 0.194800\n",
      "2017-06-10 03:32:57 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 8334183.0000 Trainning Accuracy: 0.175000 Validation Accuracy: 0.221400\n",
      "2017-06-10 03:35:00 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 4459123.0000 Trainning Accuracy: 0.200000 Validation Accuracy: 0.207800\n",
      "2017-06-10 03:37:04 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 2323365.0000 Trainning Accuracy: 0.175000 Validation Accuracy: 0.215600\n",
      "2017-06-10 03:39:08 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 1510094.3750 Trainning Accuracy: 0.225000 Validation Accuracy: 0.201000\n",
      "2017-06-10 03:41:13 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 959806.8750 Trainning Accuracy: 0.200000 Validation Accuracy: 0.210400\n",
      "2017-06-10 03:43:16 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 588256.5000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.201600\n",
      "2017-06-10 03:45:20 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 369985.0312 Trainning Accuracy: 0.225000 Validation Accuracy: 0.196200\n",
      "2017-06-10 03:47:24 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 228737.7500 Trainning Accuracy: 0.250000 Validation Accuracy: 0.179600\n",
      "2017-06-10 03:49:27 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 114151.8906 Trainning Accuracy: 0.275000 Validation Accuracy: 0.167800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .5 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 03:51:35 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 27355628.0000 Trainning Accuracy: 0.250000 Validation Accuracy: 0.270400\n",
      "2017-06-10 03:53:39 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 7614621.0000 Trainning Accuracy: 0.350000 Validation Accuracy: 0.258400\n",
      "2017-06-10 03:55:44 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 3001412.5000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.236400\n",
      "2017-06-10 03:57:48 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 1222037.7500 Trainning Accuracy: 0.225000 Validation Accuracy: 0.210200\n",
      "2017-06-10 03:59:52 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 593033.3750 Trainning Accuracy: 0.175000 Validation Accuracy: 0.183800\n",
      "2017-06-10 04:01:57 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 270837.5938 Trainning Accuracy: 0.150000 Validation Accuracy: 0.166000\n",
      "2017-06-10 04:04:01 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 121249.2266 Trainning Accuracy: 0.175000 Validation Accuracy: 0.133000\n",
      "2017-06-10 04:06:05 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 17610.4414 Trainning Accuracy: 0.125000 Validation Accuracy: 0.120400\n",
      "2017-06-10 04:08:09 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:  6024.5762 Trainning Accuracy: 0.125000 Validation Accuracy: 0.109000\n",
      "2017-06-10 04:10:13 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:   272.6840 Trainning Accuracy: 0.100000 Validation Accuracy: 0.106400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .6 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 04:12:20 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 9582000.0000 Trainning Accuracy: 0.200000 Validation Accuracy: 0.276800\n",
      "2017-06-10 04:14:23 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 2912697.0000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.286800\n",
      "2017-06-10 04:16:27 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 1146750.7500 Trainning Accuracy: 0.450000 Validation Accuracy: 0.272200\n",
      "2017-06-10 04:18:31 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 464033.0000 Trainning Accuracy: 0.425000 Validation Accuracy: 0.244000\n",
      "2017-06-10 04:20:36 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 171118.1562 Trainning Accuracy: 0.425000 Validation Accuracy: 0.210200\n",
      "2017-06-10 04:22:40 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 75457.2891 Trainning Accuracy: 0.275000 Validation Accuracy: 0.178400\n",
      "2017-06-10 04:24:45 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 22825.5352 Trainning Accuracy: 0.100000 Validation Accuracy: 0.130000\n",
      "2017-06-10 04:26:49 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  9868.1602 Trainning Accuracy: 0.050000 Validation Accuracy: 0.102800\n",
      "2017-06-10 04:28:55 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:   869.6857 Trainning Accuracy: 0.050000 Validation Accuracy: 0.098800\n",
      "2017-06-10 04:30:59 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:    43.2759 Trainning Accuracy: 0.050000 Validation Accuracy: 0.097400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .7 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 04:33:07 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 16736638.0000 Trainning Accuracy: 0.300000 Validation Accuracy: 0.306000\n",
      "2017-06-10 04:35:10 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 4424425.0000 Trainning Accuracy: 0.425000 Validation Accuracy: 0.313400\n",
      "2017-06-10 04:37:14 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 1391622.6250 Trainning Accuracy: 0.375000 Validation Accuracy: 0.284200\n",
      "2017-06-10 04:39:18 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 564475.8750 Trainning Accuracy: 0.350000 Validation Accuracy: 0.246800\n",
      "2017-06-10 04:41:23 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 242914.9219 Trainning Accuracy: 0.300000 Validation Accuracy: 0.190400\n",
      "2017-06-10 04:43:27 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 45645.7695 Trainning Accuracy: 0.250000 Validation Accuracy: 0.147800\n",
      "2017-06-10 04:45:31 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 12301.6191 Trainning Accuracy: 0.200000 Validation Accuracy: 0.122000\n",
      "2017-06-10 04:47:35 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:     2.6264 Trainning Accuracy: 0.150000 Validation Accuracy: 0.110000\n",
      "2017-06-10 04:49:38 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:     2.6256 Trainning Accuracy: 0.150000 Validation Accuracy: 0.105200\n",
      "2017-06-10 04:51:42 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:    25.9681 Trainning Accuracy: 0.125000 Validation Accuracy: 0.105000\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YYYYYYYYYYYYYYYYYYYYYYYYYYYYY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 12)\n",
      "(?, 8, 8, 48)\n",
      "(?, 3072)\n",
      "(?, 6144) with dropout\n",
      "(?, 6144) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 12)\n",
      "(?, 8, 8, 48)\n",
      "(?, 3072)\n",
      "(?, 6144) with dropout\n",
      "(?, 6144) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 12 , 'cvmp2': 48, 'cvmp3': 512}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (5, 5)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 3072*2, 'fc2' : 3072*2, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    #x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    #print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 04:53:42 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 3016759.0000 Trainning Accuracy: 0.125000 Validation Accuracy: 0.138000\n",
      "2017-06-10 04:55:36 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 1425243.3750 Trainning Accuracy: 0.200000 Validation Accuracy: 0.251000\n",
      "2017-06-10 04:57:31 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 1117745.7500 Trainning Accuracy: 0.225000 Validation Accuracy: 0.279000\n",
      "2017-06-10 04:59:26 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 755420.9375 Trainning Accuracy: 0.225000 Validation Accuracy: 0.310000\n",
      "2017-06-10 05:01:20 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 557213.3125 Trainning Accuracy: 0.300000 Validation Accuracy: 0.313200\n",
      "2017-06-10 05:03:15 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 502194.1562 Trainning Accuracy: 0.275000 Validation Accuracy: 0.323000\n",
      "2017-06-10 05:05:09 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 381322.5000 Trainning Accuracy: 0.350000 Validation Accuracy: 0.324600\n",
      "2017-06-10 05:07:03 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 300349.1562 Trainning Accuracy: 0.350000 Validation Accuracy: 0.335800\n",
      "2017-06-10 05:08:57 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 263360.5625 Trainning Accuracy: 0.375000 Validation Accuracy: 0.335400\n",
      "2017-06-10 05:10:52 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 207588.5625 Trainning Accuracy: 0.425000 Validation Accuracy: 0.330000\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .2 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 05:12:50 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1557922.2500 Trainning Accuracy: 0.225000 Validation Accuracy: 0.215600\n",
      "2017-06-10 05:14:45 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 895527.6250 Trainning Accuracy: 0.400000 Validation Accuracy: 0.294000\n",
      "2017-06-10 05:16:39 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 630823.6250 Trainning Accuracy: 0.400000 Validation Accuracy: 0.338400\n",
      "2017-06-10 05:18:34 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 488827.5938 Trainning Accuracy: 0.475000 Validation Accuracy: 0.341400\n",
      "2017-06-10 05:20:28 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 345623.4062 Trainning Accuracy: 0.500000 Validation Accuracy: 0.367600\n",
      "2017-06-10 05:22:23 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 294027.1875 Trainning Accuracy: 0.500000 Validation Accuracy: 0.374600\n",
      "2017-06-10 05:24:18 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 225290.7969 Trainning Accuracy: 0.550000 Validation Accuracy: 0.379600\n",
      "2017-06-10 05:26:13 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 183011.2500 Trainning Accuracy: 0.550000 Validation Accuracy: 0.382800\n",
      "2017-06-10 05:28:06 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 146533.3438 Trainning Accuracy: 0.550000 Validation Accuracy: 0.383600\n",
      "2017-06-10 05:30:01 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 118056.7266 Trainning Accuracy: 0.575000 Validation Accuracy: 0.385400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .3 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 05:31:59 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1570541.6250 Trainning Accuracy: 0.225000 Validation Accuracy: 0.284000\n",
      "2017-06-10 05:33:54 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 988662.8750 Trainning Accuracy: 0.300000 Validation Accuracy: 0.327600\n",
      "2017-06-10 05:35:49 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 602944.5000 Trainning Accuracy: 0.350000 Validation Accuracy: 0.344600\n",
      "2017-06-10 05:37:44 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 405215.5938 Trainning Accuracy: 0.450000 Validation Accuracy: 0.353400\n",
      "2017-06-10 05:39:39 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 232590.2969 Trainning Accuracy: 0.475000 Validation Accuracy: 0.371800\n",
      "2017-06-10 05:41:34 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 160420.9219 Trainning Accuracy: 0.525000 Validation Accuracy: 0.373600\n",
      "2017-06-10 05:43:28 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 121627.2656 Trainning Accuracy: 0.575000 Validation Accuracy: 0.385800\n",
      "2017-06-10 05:45:23 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 93864.2109 Trainning Accuracy: 0.550000 Validation Accuracy: 0.381400\n",
      "2017-06-10 05:47:18 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 75887.3906 Trainning Accuracy: 0.525000 Validation Accuracy: 0.376200\n",
      "2017-06-10 05:49:13 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 63690.7266 Trainning Accuracy: 0.625000 Validation Accuracy: 0.360600\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .33 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 05:51:10 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 2967939.0000 Trainning Accuracy: 0.125000 Validation Accuracy: 0.233800\n",
      "2017-06-10 05:53:04 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 1537607.2500 Trainning Accuracy: 0.250000 Validation Accuracy: 0.310200\n",
      "2017-06-10 05:54:59 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 897726.1250 Trainning Accuracy: 0.275000 Validation Accuracy: 0.352400\n",
      "2017-06-10 05:56:54 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 603405.1875 Trainning Accuracy: 0.375000 Validation Accuracy: 0.379400\n",
      "2017-06-10 05:58:48 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 394068.6875 Trainning Accuracy: 0.500000 Validation Accuracy: 0.393600\n",
      "2017-06-10 06:00:43 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 301709.0938 Trainning Accuracy: 0.550000 Validation Accuracy: 0.396000\n",
      "2017-06-10 06:02:37 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 212431.1719 Trainning Accuracy: 0.600000 Validation Accuracy: 0.400200\n",
      "2017-06-10 06:04:32 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 168187.0156 Trainning Accuracy: 0.575000 Validation Accuracy: 0.403600\n",
      "2017-06-10 06:06:27 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 144509.4688 Trainning Accuracy: 0.625000 Validation Accuracy: 0.408800\n",
      "2017-06-10 06:08:22 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 115549.0859 Trainning Accuracy: 0.625000 Validation Accuracy: 0.410800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .4 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 06:10:19 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1769114.7500 Trainning Accuracy: 0.200000 Validation Accuracy: 0.293600\n",
      "2017-06-10 06:12:14 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 1037014.1875 Trainning Accuracy: 0.300000 Validation Accuracy: 0.353000\n",
      "2017-06-10 06:14:09 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 608924.3750 Trainning Accuracy: 0.425000 Validation Accuracy: 0.380600\n",
      "2017-06-10 06:16:05 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 341047.6562 Trainning Accuracy: 0.475000 Validation Accuracy: 0.390800\n",
      "2017-06-10 06:17:59 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 229936.0000 Trainning Accuracy: 0.600000 Validation Accuracy: 0.396000\n",
      "2017-06-10 06:19:55 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 131703.2812 Trainning Accuracy: 0.650000 Validation Accuracy: 0.399800\n",
      "2017-06-10 06:21:49 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 97716.8906 Trainning Accuracy: 0.650000 Validation Accuracy: 0.404600\n",
      "2017-06-10 06:23:43 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 66879.2344 Trainning Accuracy: 0.725000 Validation Accuracy: 0.400600\n",
      "2017-06-10 06:25:38 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 55971.1797 Trainning Accuracy: 0.725000 Validation Accuracy: 0.399000\n",
      "2017-06-10 06:27:32 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 46778.1133 Trainning Accuracy: 0.700000 Validation Accuracy: 0.395400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .5 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 06:29:30 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1484479.2500 Trainning Accuracy: 0.350000 Validation Accuracy: 0.307400\n",
      "2017-06-10 06:31:24 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 618785.0000 Trainning Accuracy: 0.450000 Validation Accuracy: 0.372600\n",
      "2017-06-10 06:33:19 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 368114.5938 Trainning Accuracy: 0.500000 Validation Accuracy: 0.392200\n",
      "2017-06-10 06:35:14 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 264550.3438 Trainning Accuracy: 0.600000 Validation Accuracy: 0.393800\n",
      "2017-06-10 06:37:08 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 155583.4375 Trainning Accuracy: 0.575000 Validation Accuracy: 0.408800\n",
      "2017-06-10 06:39:03 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 92140.1406 Trainning Accuracy: 0.650000 Validation Accuracy: 0.408800\n",
      "2017-06-10 06:40:58 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 58848.1445 Trainning Accuracy: 0.750000 Validation Accuracy: 0.417400\n",
      "2017-06-10 06:42:53 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 51112.7227 Trainning Accuracy: 0.775000 Validation Accuracy: 0.418600\n",
      "2017-06-10 06:44:48 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 38882.5312 Trainning Accuracy: 0.750000 Validation Accuracy: 0.411000\n",
      "2017-06-10 06:46:43 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 29958.0312 Trainning Accuracy: 0.800000 Validation Accuracy: 0.421800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .6 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 06:48:40 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1163486.1250 Trainning Accuracy: 0.300000 Validation Accuracy: 0.323800\n",
      "2017-06-10 06:50:35 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 526090.6875 Trainning Accuracy: 0.500000 Validation Accuracy: 0.383600\n",
      "2017-06-10 06:52:30 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 268074.6875 Trainning Accuracy: 0.625000 Validation Accuracy: 0.406200\n",
      "2017-06-10 06:54:25 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 174913.5312 Trainning Accuracy: 0.650000 Validation Accuracy: 0.420800\n",
      "2017-06-10 06:56:20 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 109825.5469 Trainning Accuracy: 0.725000 Validation Accuracy: 0.425600\n",
      "2017-06-10 06:58:14 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 80780.7734 Trainning Accuracy: 0.825000 Validation Accuracy: 0.431800\n",
      "2017-06-10 07:00:10 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 48434.0312 Trainning Accuracy: 0.800000 Validation Accuracy: 0.429600\n",
      "2017-06-10 07:02:04 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 28822.8223 Trainning Accuracy: 0.850000 Validation Accuracy: 0.435000\n",
      "2017-06-10 07:03:59 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 20105.2715 Trainning Accuracy: 0.850000 Validation Accuracy: 0.438400\n",
      "2017-06-10 07:05:53 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 13989.6670 Trainning Accuracy: 0.925000 Validation Accuracy: 0.440400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .7 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes novos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 12)\n",
      "(?, 8, 8, 48)\n",
      "(?, 3072)\n",
      "(?, 6144) with dropout\n",
      "(?, 6144) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 12)\n",
      "(?, 8, 8, 48)\n",
      "(?, 3072)\n",
      "(?, 6144) with dropout\n",
      "(?, 6144) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 12 , 'cvmp2': 48, 'cvmp3': 512}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (3, 3), 'cvmp3': (5, 5)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 3072*2, 'fc2' : 3072*2, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    #x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    #print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 10:12:17 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 875048.0000 Trainning Accuracy: 0.200000 Validation Accuracy: 0.223400\n",
      "2017-06-10 10:14:21 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 546126.3125 Trainning Accuracy: 0.225000 Validation Accuracy: 0.265800\n",
      "2017-06-10 10:16:13 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 388774.1250 Trainning Accuracy: 0.200000 Validation Accuracy: 0.278200\n",
      "2017-06-10 10:18:10 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 267627.0000 Trainning Accuracy: 0.200000 Validation Accuracy: 0.283600\n",
      "2017-06-10 10:20:00 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 199706.4688 Trainning Accuracy: 0.225000 Validation Accuracy: 0.285000\n",
      "2017-06-10 10:21:55 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 154257.2812 Trainning Accuracy: 0.250000 Validation Accuracy: 0.297200\n",
      "2017-06-10 10:23:47 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 126396.1719 Trainning Accuracy: 0.300000 Validation Accuracy: 0.297600\n",
      "2017-06-10 10:25:39 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 90723.1328 Trainning Accuracy: 0.375000 Validation Accuracy: 0.300000\n",
      "2017-06-10 10:27:29 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 63167.9883 Trainning Accuracy: 0.425000 Validation Accuracy: 0.304000\n",
      "2017-06-10 10:29:20 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 48941.6016 Trainning Accuracy: 0.400000 Validation Accuracy: 0.294800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .2 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 10:31:15 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1699732.6250 Trainning Accuracy: 0.175000 Validation Accuracy: 0.226400\n",
      "2017-06-10 10:33:05 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 980607.3125 Trainning Accuracy: 0.200000 Validation Accuracy: 0.299800\n",
      "2017-06-10 10:34:58 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 655130.0625 Trainning Accuracy: 0.200000 Validation Accuracy: 0.332600\n",
      "2017-06-10 10:36:53 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 440072.9062 Trainning Accuracy: 0.300000 Validation Accuracy: 0.341600\n",
      "2017-06-10 10:38:45 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 293625.8438 Trainning Accuracy: 0.350000 Validation Accuracy: 0.363800\n",
      "2017-06-10 10:40:43 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 193351.5312 Trainning Accuracy: 0.400000 Validation Accuracy: 0.381600\n",
      "2017-06-10 10:42:33 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 175139.4219 Trainning Accuracy: 0.425000 Validation Accuracy: 0.386200\n",
      "2017-06-10 10:44:24 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 139035.3281 Trainning Accuracy: 0.500000 Validation Accuracy: 0.392400\n",
      "2017-06-10 10:46:21 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 99132.7344 Trainning Accuracy: 0.500000 Validation Accuracy: 0.395200\n",
      "2017-06-10 10:48:15 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 79925.5156 Trainning Accuracy: 0.550000 Validation Accuracy: 0.401200\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .3 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 10:50:10 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 498386.3438 Trainning Accuracy: 0.125000 Validation Accuracy: 0.226600\n",
      "2017-06-10 10:52:00 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 214962.9688 Trainning Accuracy: 0.275000 Validation Accuracy: 0.274400\n",
      "2017-06-10 10:53:52 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 143021.8281 Trainning Accuracy: 0.425000 Validation Accuracy: 0.300000\n",
      "2017-06-10 10:55:44 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 107287.1719 Trainning Accuracy: 0.375000 Validation Accuracy: 0.312000\n",
      "2017-06-10 10:57:34 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 76999.5625 Trainning Accuracy: 0.425000 Validation Accuracy: 0.321400\n",
      "2017-06-10 10:59:24 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 64431.9922 Trainning Accuracy: 0.400000 Validation Accuracy: 0.312400\n",
      "2017-06-10 11:01:14 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 43418.7578 Trainning Accuracy: 0.500000 Validation Accuracy: 0.324600\n",
      "2017-06-10 11:03:05 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 34024.4805 Trainning Accuracy: 0.475000 Validation Accuracy: 0.315200\n",
      "2017-06-10 11:04:56 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 26396.0938 Trainning Accuracy: 0.525000 Validation Accuracy: 0.319200\n",
      "2017-06-10 11:06:46 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 23399.7461 Trainning Accuracy: 0.525000 Validation Accuracy: 0.313600\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .33 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 11:08:41 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1838408.0000 Trainning Accuracy: 0.125000 Validation Accuracy: 0.233400\n",
      "2017-06-10 11:10:32 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 773584.5000 Trainning Accuracy: 0.325000 Validation Accuracy: 0.308800\n",
      "2017-06-10 11:12:22 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 423525.8438 Trainning Accuracy: 0.400000 Validation Accuracy: 0.370800\n",
      "2017-06-10 11:14:12 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 218766.2031 Trainning Accuracy: 0.475000 Validation Accuracy: 0.384200\n",
      "2017-06-10 11:16:03 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 167697.7969 Trainning Accuracy: 0.525000 Validation Accuracy: 0.388400\n",
      "2017-06-10 11:17:54 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 115043.2969 Trainning Accuracy: 0.600000 Validation Accuracy: 0.405400\n",
      "2017-06-10 11:19:45 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 76544.4688 Trainning Accuracy: 0.600000 Validation Accuracy: 0.428400\n",
      "2017-06-10 11:21:35 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 65663.4688 Trainning Accuracy: 0.650000 Validation Accuracy: 0.419600\n",
      "2017-06-10 11:23:26 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 37632.7734 Trainning Accuracy: 0.750000 Validation Accuracy: 0.424000\n",
      "2017-06-10 11:25:16 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 25251.9238 Trainning Accuracy: 0.800000 Validation Accuracy: 0.420800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .5 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 11:27:12 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1130312.2500 Trainning Accuracy: 0.275000 Validation Accuracy: 0.275200\n",
      "2017-06-10 11:29:12 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 621821.8750 Trainning Accuracy: 0.375000 Validation Accuracy: 0.352400\n",
      "2017-06-10 11:31:03 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 351785.9375 Trainning Accuracy: 0.425000 Validation Accuracy: 0.377800\n",
      "2017-06-10 11:32:54 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 200404.1094 Trainning Accuracy: 0.625000 Validation Accuracy: 0.414600\n",
      "2017-06-10 11:34:47 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 113532.8516 Trainning Accuracy: 0.675000 Validation Accuracy: 0.416000\n",
      "2017-06-10 11:36:40 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 88588.1094 Trainning Accuracy: 0.675000 Validation Accuracy: 0.413200\n",
      "2017-06-10 11:38:38 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 68288.4844 Trainning Accuracy: 0.725000 Validation Accuracy: 0.411400\n",
      "2017-06-10 11:40:32 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 51337.9453 Trainning Accuracy: 0.750000 Validation Accuracy: 0.426400\n",
      "2017-06-10 11:42:26 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 39233.7383 Trainning Accuracy: 0.800000 Validation Accuracy: 0.439200\n",
      "2017-06-10 11:44:20 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 27687.9805 Trainning Accuracy: 0.800000 Validation Accuracy: 0.449200\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .5 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 11:46:16 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1179835.2500 Trainning Accuracy: 0.275000 Validation Accuracy: 0.295400\n",
      "2017-06-10 11:48:08 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 608733.5625 Trainning Accuracy: 0.475000 Validation Accuracy: 0.348400\n",
      "2017-06-10 11:50:01 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 314849.9688 Trainning Accuracy: 0.525000 Validation Accuracy: 0.397600\n",
      "2017-06-10 11:51:54 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 198945.4844 Trainning Accuracy: 0.525000 Validation Accuracy: 0.419000\n",
      "2017-06-10 11:53:46 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 126192.1094 Trainning Accuracy: 0.675000 Validation Accuracy: 0.416400\n",
      "2017-06-10 11:55:40 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 87542.4844 Trainning Accuracy: 0.675000 Validation Accuracy: 0.436200\n",
      "2017-06-10 11:57:34 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 55310.7109 Trainning Accuracy: 0.725000 Validation Accuracy: 0.441000\n",
      "2017-06-10 11:59:38 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 43131.1758 Trainning Accuracy: 0.775000 Validation Accuracy: 0.444200\n",
      "2017-06-10 12:01:31 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 35466.4922 Trainning Accuracy: 0.825000 Validation Accuracy: 0.441800\n",
      "2017-06-10 12:03:25 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 17528.0879 Trainning Accuracy: 0.900000 Validation Accuracy: 0.453600\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .6 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 12:05:24 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1212215.2500 Trainning Accuracy: 0.200000 Validation Accuracy: 0.331800\n",
      "2017-06-10 12:07:16 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 664988.0000 Trainning Accuracy: 0.475000 Validation Accuracy: 0.361800\n",
      "2017-06-10 12:09:08 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 352249.8750 Trainning Accuracy: 0.525000 Validation Accuracy: 0.402400\n",
      "2017-06-10 12:11:01 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 216989.7812 Trainning Accuracy: 0.600000 Validation Accuracy: 0.424800\n",
      "2017-06-10 12:12:52 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 146257.1875 Trainning Accuracy: 0.575000 Validation Accuracy: 0.423200\n",
      "2017-06-10 12:14:44 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 102551.3594 Trainning Accuracy: 0.675000 Validation Accuracy: 0.440000\n",
      "2017-06-10 12:16:35 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 87835.8125 Trainning Accuracy: 0.775000 Validation Accuracy: 0.441400\n",
      "2017-06-10 12:18:26 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 61363.9258 Trainning Accuracy: 0.775000 Validation Accuracy: 0.435400\n",
      "2017-06-10 12:20:18 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 37530.7578 Trainning Accuracy: 0.775000 Validation Accuracy: 0.436400\n",
      "2017-06-10 12:22:09 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 23932.6758 Trainning Accuracy: 0.850000 Validation Accuracy: 0.441200\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .7 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 12:24:03 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1031516.3125 Trainning Accuracy: 0.150000 Validation Accuracy: 0.295800\n",
      "2017-06-10 12:25:55 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 419869.4375 Trainning Accuracy: 0.400000 Validation Accuracy: 0.369200\n",
      "2017-06-10 12:27:46 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 217021.8438 Trainning Accuracy: 0.525000 Validation Accuracy: 0.405000\n",
      "2017-06-10 12:29:38 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 141182.6719 Trainning Accuracy: 0.625000 Validation Accuracy: 0.412600\n",
      "2017-06-10 12:31:29 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 79036.6328 Trainning Accuracy: 0.675000 Validation Accuracy: 0.421200\n",
      "2017-06-10 12:33:20 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 54382.2422 Trainning Accuracy: 0.750000 Validation Accuracy: 0.425000\n",
      "2017-06-10 12:35:12 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 25978.1016 Trainning Accuracy: 0.825000 Validation Accuracy: 0.432000\n",
      "2017-06-10 12:37:03 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 15437.2998 Trainning Accuracy: 0.850000 Validation Accuracy: 0.428000\n",
      "2017-06-10 12:38:54 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 11323.6201 Trainning Accuracy: 0.875000 Validation Accuracy: 0.430200\n",
      "2017-06-10 12:40:45 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:  6808.6748 Trainning Accuracy: 0.900000 Validation Accuracy: 0.422800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim dos Outros Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novos Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 12)\n",
      "(?, 8, 8, 48)\n",
      "(?, 4, 4, 192)\n",
      "(?, 3072)\n",
      "(?, 6144) with dropout\n",
      "(?, 6144) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 12)\n",
      "(?, 8, 8, 48)\n",
      "(?, 4, 4, 192)\n",
      "(?, 3072)\n",
      "(?, 6144) with dropout\n",
      "(?, 6144) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 12 , 'cvmp2': 48, 'cvmp3': 192}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (3, 3), 'cvmp3': (2, 2)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 3072*2, 'fc2' : 3072*2, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-11 15:51:01 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 5784795.0000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.318400\n",
      "2017-06-11 15:53:00 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 2083099.7500 Trainning Accuracy: 0.550000 Validation Accuracy: 0.396800\n",
      "2017-06-11 15:54:59 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 1131857.7500 Trainning Accuracy: 0.675000 Validation Accuracy: 0.428000\n",
      "2017-06-11 15:56:58 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 902039.3750 Trainning Accuracy: 0.675000 Validation Accuracy: 0.432400\n",
      "2017-06-11 15:58:57 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 411378.2188 Trainning Accuracy: 0.850000 Validation Accuracy: 0.442800\n",
      "2017-06-11 16:00:59 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 312909.4375 Trainning Accuracy: 0.825000 Validation Accuracy: 0.452800\n",
      "2017-06-11 16:03:02 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 91881.0781 Trainning Accuracy: 0.925000 Validation Accuracy: 0.463800\n",
      "2017-06-11 16:05:04 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 98360.7891 Trainning Accuracy: 0.875000 Validation Accuracy: 0.475800\n",
      "2017-06-11 16:07:03 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 64954.6797 Trainning Accuracy: 0.900000 Validation Accuracy: 0.472400\n",
      "2017-06-11 16:09:02 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 38030.0859 Trainning Accuracy: 0.950000 Validation Accuracy: 0.467800\n",
      "2017-06-11 16:11:02 - Epoch 11, CIFAR-10 Batch 1:  Training Loss: 18923.9062 Trainning Accuracy: 0.950000 Validation Accuracy: 0.470800\n",
      "2017-06-11 16:13:01 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:  3540.8062 Trainning Accuracy: 0.950000 Validation Accuracy: 0.472400\n",
      "2017-06-11 16:15:01 - Epoch 13, CIFAR-10 Batch 1:  Training Loss: 17815.6367 Trainning Accuracy: 0.950000 Validation Accuracy: 0.467000\n",
      "2017-06-11 16:17:21 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.478200\n",
      "2017-06-11 16:19:19 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:   583.5344 Trainning Accuracy: 0.975000 Validation Accuracy: 0.479800\n",
      "2017-06-11 16:21:17 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.483400\n",
      "2017-06-11 16:23:20 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.481600\n",
      "2017-06-11 16:25:19 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.484000\n",
      "2017-06-11 16:27:27 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.488000\n",
      "2017-06-11 16:29:30 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.487400\n",
      "2017-06-11 16:31:31 - Epoch 21, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.494000\n",
      "2017-06-11 16:33:38 - Epoch 22, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.484600\n",
      "2017-06-11 16:35:42 - Epoch 23, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.488400\n",
      "2017-06-11 16:37:52 - Epoch 24, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.491200\n",
      "2017-06-11 16:39:56 - Epoch 25, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.487200\n",
      "2017-06-11 16:41:56 - Epoch 26, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.490400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-710e2bc6a508>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-81050f085b79>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# DONE: Implement Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \"\"\"\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 30\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-11 20:40:11 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 9890714.0000 Trainning Accuracy: 0.250000 Validation Accuracy: 0.258800\n",
      "2017-06-11 20:42:08 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 3779512.0000 Trainning Accuracy: 0.325000 Validation Accuracy: 0.363800\n",
      "2017-06-11 20:44:05 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 2256715.7500 Trainning Accuracy: 0.450000 Validation Accuracy: 0.400600\n",
      "2017-06-11 20:46:03 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 1161868.6250 Trainning Accuracy: 0.550000 Validation Accuracy: 0.416800\n",
      "2017-06-11 20:48:01 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 723495.5625 Trainning Accuracy: 0.625000 Validation Accuracy: 0.433200\n",
      "2017-06-11 20:49:59 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 406802.3438 Trainning Accuracy: 0.800000 Validation Accuracy: 0.445200\n",
      "2017-06-11 20:51:56 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 372857.9062 Trainning Accuracy: 0.800000 Validation Accuracy: 0.444000\n",
      "2017-06-11 20:53:53 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 265380.0312 Trainning Accuracy: 0.800000 Validation Accuracy: 0.454600\n",
      "2017-06-11 20:55:50 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 151828.1562 Trainning Accuracy: 0.850000 Validation Accuracy: 0.465200\n",
      "2017-06-11 20:57:47 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 98291.7344 Trainning Accuracy: 0.850000 Validation Accuracy: 0.474200\n",
      "2017-06-11 20:59:44 - Epoch 11, CIFAR-10 Batch 1:  Training Loss: 29334.3633 Trainning Accuracy: 0.925000 Validation Accuracy: 0.468800\n",
      "2017-06-11 21:01:49 - Epoch 12, CIFAR-10 Batch 1:  Training Loss: 36187.8008 Trainning Accuracy: 0.875000 Validation Accuracy: 0.451400\n",
      "2017-06-11 21:03:46 - Epoch 13, CIFAR-10 Batch 1:  Training Loss: 21392.6738 Trainning Accuracy: 0.925000 Validation Accuracy: 0.467000\n",
      "2017-06-11 21:05:44 - Epoch 14, CIFAR-10 Batch 1:  Training Loss: 20547.3184 Trainning Accuracy: 0.925000 Validation Accuracy: 0.462200\n",
      "2017-06-11 21:07:42 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.466600\n",
      "2017-06-11 21:09:40 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:  3813.3823 Trainning Accuracy: 0.975000 Validation Accuracy: 0.470200\n",
      "2017-06-11 21:11:37 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.470400\n",
      "2017-06-11 21:13:34 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:  4210.8140 Trainning Accuracy: 0.975000 Validation Accuracy: 0.469400\n",
      "2017-06-11 21:15:31 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.472000\n",
      "2017-06-11 21:17:28 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.476600\n",
      "2017-06-11 21:19:25 - Epoch 21, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.478400\n",
      "2017-06-11 21:21:23 - Epoch 22, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.480400\n",
      "2017-06-11 21:23:19 - Epoch 23, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.483800\n",
      "2017-06-11 21:25:15 - Epoch 24, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.489800\n",
      "2017-06-11 21:27:12 - Epoch 25, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.484000\n",
      "2017-06-11 21:29:09 - Epoch 26, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.480400\n",
      "2017-06-11 21:31:06 - Epoch 27, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.485600\n",
      "2017-06-11 21:33:03 - Epoch 28, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.485200\n",
      "2017-06-11 21:35:00 - Epoch 29, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.489000\n",
      "2017-06-11 21:36:57 - Epoch 30, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.486400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 30\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .7 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-11 21:38:57 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 11334460.0000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.300600\n",
      "2017-06-11 21:40:54 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 6131644.0000 Trainning Accuracy: 0.325000 Validation Accuracy: 0.355400\n",
      "2017-06-11 21:42:51 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 2851020.2500 Trainning Accuracy: 0.400000 Validation Accuracy: 0.399000\n",
      "2017-06-11 21:44:51 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 1634084.0000 Trainning Accuracy: 0.500000 Validation Accuracy: 0.409200\n",
      "2017-06-11 21:46:50 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 1135002.6250 Trainning Accuracy: 0.600000 Validation Accuracy: 0.429200\n",
      "2017-06-11 21:48:49 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 842346.5000 Trainning Accuracy: 0.600000 Validation Accuracy: 0.427800\n",
      "2017-06-11 21:50:48 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 612459.6250 Trainning Accuracy: 0.625000 Validation Accuracy: 0.447200\n",
      "2017-06-11 21:52:45 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 524019.3438 Trainning Accuracy: 0.725000 Validation Accuracy: 0.447800\n",
      "2017-06-11 21:54:42 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 336006.4062 Trainning Accuracy: 0.800000 Validation Accuracy: 0.452200\n",
      "2017-06-11 21:56:39 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 247575.4531 Trainning Accuracy: 0.775000 Validation Accuracy: 0.455800\n",
      "2017-06-11 21:58:36 - Epoch 11, CIFAR-10 Batch 1:  Training Loss: 169711.7188 Trainning Accuracy: 0.825000 Validation Accuracy: 0.454600\n",
      "2017-06-11 22:00:33 - Epoch 12, CIFAR-10 Batch 1:  Training Loss: 118438.3984 Trainning Accuracy: 0.875000 Validation Accuracy: 0.464800\n",
      "2017-06-11 22:02:30 - Epoch 13, CIFAR-10 Batch 1:  Training Loss: 101891.4844 Trainning Accuracy: 0.875000 Validation Accuracy: 0.474000\n",
      "2017-06-11 22:04:27 - Epoch 14, CIFAR-10 Batch 1:  Training Loss: 56414.0000 Trainning Accuracy: 0.875000 Validation Accuracy: 0.469600\n",
      "2017-06-11 22:06:24 - Epoch 15, CIFAR-10 Batch 1:  Training Loss: 42183.6367 Trainning Accuracy: 0.875000 Validation Accuracy: 0.472600\n",
      "2017-06-11 22:08:21 - Epoch 16, CIFAR-10 Batch 1:  Training Loss: 40205.0820 Trainning Accuracy: 0.900000 Validation Accuracy: 0.478600\n",
      "2017-06-11 22:10:19 - Epoch 17, CIFAR-10 Batch 1:  Training Loss: 23178.9102 Trainning Accuracy: 0.925000 Validation Accuracy: 0.475000\n",
      "2017-06-11 22:12:16 - Epoch 18, CIFAR-10 Batch 1:  Training Loss: 23199.5469 Trainning Accuracy: 0.950000 Validation Accuracy: 0.476200\n",
      "2017-06-11 22:14:13 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.481800\n",
      "2017-06-11 22:16:13 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:  1588.0687 Trainning Accuracy: 0.975000 Validation Accuracy: 0.478800\n",
      "2017-06-11 22:18:12 - Epoch 21, CIFAR-10 Batch 1:  Training Loss: 10277.6777 Trainning Accuracy: 0.975000 Validation Accuracy: 0.476800\n",
      "2017-06-11 22:20:10 - Epoch 22, CIFAR-10 Batch 1:  Training Loss: 10408.2090 Trainning Accuracy: 0.975000 Validation Accuracy: 0.479800\n",
      "2017-06-11 22:22:06 - Epoch 23, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.479400\n",
      "2017-06-11 22:24:03 - Epoch 24, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.477600\n",
      "2017-06-11 22:26:01 - Epoch 25, CIFAR-10 Batch 1:  Training Loss:  9258.5352 Trainning Accuracy: 0.975000 Validation Accuracy: 0.483400\n",
      "2017-06-11 22:27:57 - Epoch 26, CIFAR-10 Batch 1:  Training Loss:  4055.1992 Trainning Accuracy: 0.975000 Validation Accuracy: 0.477800\n",
      "2017-06-11 22:29:54 - Epoch 27, CIFAR-10 Batch 1:  Training Loss:   865.2344 Trainning Accuracy: 0.975000 Validation Accuracy: 0.475600\n",
      "2017-06-11 22:31:51 - Epoch 28, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.479400\n",
      "2017-06-11 22:33:48 - Epoch 29, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.475200\n",
      "2017-06-11 22:35:45 - Epoch 30, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.479600\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 30\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .6 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-11 22:37:45 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 12507496.0000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.261800\n",
      "2017-06-11 22:39:43 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 5514729.0000 Trainning Accuracy: 0.425000 Validation Accuracy: 0.360200\n",
      "2017-06-11 22:41:42 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 2942240.7500 Trainning Accuracy: 0.475000 Validation Accuracy: 0.397800\n",
      "2017-06-11 22:43:40 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 1867571.6250 Trainning Accuracy: 0.525000 Validation Accuracy: 0.418600\n",
      "2017-06-11 22:45:39 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 1260735.1250 Trainning Accuracy: 0.625000 Validation Accuracy: 0.414000\n",
      "2017-06-11 22:47:37 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 959584.3750 Trainning Accuracy: 0.575000 Validation Accuracy: 0.432200\n",
      "2017-06-11 22:49:35 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 670252.3750 Trainning Accuracy: 0.650000 Validation Accuracy: 0.435800\n",
      "2017-06-11 22:51:32 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 576557.4375 Trainning Accuracy: 0.675000 Validation Accuracy: 0.437800\n",
      "2017-06-11 22:53:28 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 421688.0000 Trainning Accuracy: 0.700000 Validation Accuracy: 0.439600\n",
      "2017-06-11 22:55:25 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 266658.7188 Trainning Accuracy: 0.750000 Validation Accuracy: 0.446200\n",
      "2017-06-11 22:57:22 - Epoch 11, CIFAR-10 Batch 1:  Training Loss: 188401.7656 Trainning Accuracy: 0.775000 Validation Accuracy: 0.450800\n",
      "2017-06-11 22:59:19 - Epoch 12, CIFAR-10 Batch 1:  Training Loss: 121218.5781 Trainning Accuracy: 0.800000 Validation Accuracy: 0.450800\n",
      "2017-06-11 23:01:16 - Epoch 13, CIFAR-10 Batch 1:  Training Loss: 83273.9219 Trainning Accuracy: 0.900000 Validation Accuracy: 0.455400\n",
      "2017-06-11 23:03:12 - Epoch 14, CIFAR-10 Batch 1:  Training Loss: 54096.7734 Trainning Accuracy: 0.900000 Validation Accuracy: 0.457200\n",
      "2017-06-11 23:05:09 - Epoch 15, CIFAR-10 Batch 1:  Training Loss: 46164.2422 Trainning Accuracy: 0.900000 Validation Accuracy: 0.461600\n",
      "2017-06-11 23:07:06 - Epoch 16, CIFAR-10 Batch 1:  Training Loss: 37039.8867 Trainning Accuracy: 0.875000 Validation Accuracy: 0.458200\n",
      "2017-06-11 23:09:03 - Epoch 17, CIFAR-10 Batch 1:  Training Loss: 30932.1934 Trainning Accuracy: 0.925000 Validation Accuracy: 0.457400\n",
      "2017-06-11 23:10:59 - Epoch 18, CIFAR-10 Batch 1:  Training Loss: 18277.6621 Trainning Accuracy: 0.875000 Validation Accuracy: 0.456200\n",
      "2017-06-11 23:12:56 - Epoch 19, CIFAR-10 Batch 1:  Training Loss: 15275.5723 Trainning Accuracy: 0.950000 Validation Accuracy: 0.459200\n",
      "2017-06-11 23:14:53 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:  6541.6929 Trainning Accuracy: 0.950000 Validation Accuracy: 0.452800\n",
      "2017-06-11 23:16:49 - Epoch 21, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.454200\n",
      "2017-06-11 23:18:45 - Epoch 22, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.453400\n",
      "2017-06-11 23:20:42 - Epoch 23, CIFAR-10 Batch 1:  Training Loss:  1206.5699 Trainning Accuracy: 0.975000 Validation Accuracy: 0.447800\n",
      "2017-06-11 23:22:39 - Epoch 24, CIFAR-10 Batch 1:  Training Loss:  2837.2839 Trainning Accuracy: 0.975000 Validation Accuracy: 0.445400\n",
      "2017-06-11 23:24:37 - Epoch 25, CIFAR-10 Batch 1:  Training Loss:   151.8232 Trainning Accuracy: 0.975000 Validation Accuracy: 0.434200\n",
      "2017-06-11 23:26:33 - Epoch 26, CIFAR-10 Batch 1:  Training Loss:  2191.1462 Trainning Accuracy: 0.975000 Validation Accuracy: 0.438800\n",
      "2017-06-11 23:28:30 - Epoch 27, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.444800\n",
      "2017-06-11 23:30:27 - Epoch 28, CIFAR-10 Batch 1:  Training Loss:   249.9602 Trainning Accuracy: 0.975000 Validation Accuracy: 0.435400\n",
      "2017-06-11 23:32:24 - Epoch 29, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.430600\n",
      "2017-06-11 23:34:20 - Epoch 30, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.428800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 30\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .5 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-11 23:36:21 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 15446467.0000 Trainning Accuracy: 0.200000 Validation Accuracy: 0.215400\n",
      "2017-06-11 23:38:18 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 8506596.0000 Trainning Accuracy: 0.150000 Validation Accuracy: 0.300000\n",
      "2017-06-11 23:40:16 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 4671657.0000 Trainning Accuracy: 0.275000 Validation Accuracy: 0.335800\n",
      "2017-06-11 23:42:13 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 3021109.5000 Trainning Accuracy: 0.400000 Validation Accuracy: 0.367400\n",
      "2017-06-11 23:44:11 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 2027746.2500 Trainning Accuracy: 0.425000 Validation Accuracy: 0.376600\n",
      "2017-06-11 23:46:07 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 1587222.2500 Trainning Accuracy: 0.425000 Validation Accuracy: 0.398000\n",
      "2017-06-11 23:48:04 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 1122714.0000 Trainning Accuracy: 0.475000 Validation Accuracy: 0.401000\n",
      "2017-06-11 23:50:01 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 863743.1250 Trainning Accuracy: 0.500000 Validation Accuracy: 0.399200\n",
      "2017-06-11 23:51:58 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 666312.6250 Trainning Accuracy: 0.550000 Validation Accuracy: 0.410400\n",
      "2017-06-11 23:53:56 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 519707.0938 Trainning Accuracy: 0.525000 Validation Accuracy: 0.423000\n",
      "2017-06-11 23:55:53 - Epoch 11, CIFAR-10 Batch 1:  Training Loss: 401472.4375 Trainning Accuracy: 0.625000 Validation Accuracy: 0.421200\n",
      "2017-06-11 23:57:50 - Epoch 12, CIFAR-10 Batch 1:  Training Loss: 305186.2812 Trainning Accuracy: 0.650000 Validation Accuracy: 0.419800\n",
      "2017-06-11 23:59:47 - Epoch 13, CIFAR-10 Batch 1:  Training Loss: 228631.8281 Trainning Accuracy: 0.700000 Validation Accuracy: 0.420000\n",
      "2017-06-12 00:01:44 - Epoch 14, CIFAR-10 Batch 1:  Training Loss: 198275.2812 Trainning Accuracy: 0.675000 Validation Accuracy: 0.416000\n",
      "2017-06-12 00:03:41 - Epoch 15, CIFAR-10 Batch 1:  Training Loss: 120202.8125 Trainning Accuracy: 0.700000 Validation Accuracy: 0.409800\n",
      "2017-06-12 00:05:38 - Epoch 16, CIFAR-10 Batch 1:  Training Loss: 112126.2031 Trainning Accuracy: 0.650000 Validation Accuracy: 0.402200\n",
      "2017-06-12 00:07:35 - Epoch 17, CIFAR-10 Batch 1:  Training Loss: 88943.3594 Trainning Accuracy: 0.700000 Validation Accuracy: 0.388600\n",
      "2017-06-12 00:09:32 - Epoch 18, CIFAR-10 Batch 1:  Training Loss: 59040.2500 Trainning Accuracy: 0.700000 Validation Accuracy: 0.380600\n",
      "2017-06-12 00:11:29 - Epoch 19, CIFAR-10 Batch 1:  Training Loss: 24332.0215 Trainning Accuracy: 0.725000 Validation Accuracy: 0.365200\n",
      "2017-06-12 00:13:25 - Epoch 20, CIFAR-10 Batch 1:  Training Loss: 22136.2090 Trainning Accuracy: 0.750000 Validation Accuracy: 0.349000\n",
      "2017-06-12 00:15:22 - Epoch 21, CIFAR-10 Batch 1:  Training Loss: 20107.2598 Trainning Accuracy: 0.675000 Validation Accuracy: 0.332000\n",
      "2017-06-12 00:17:20 - Epoch 22, CIFAR-10 Batch 1:  Training Loss: 22249.4941 Trainning Accuracy: 0.675000 Validation Accuracy: 0.316600\n",
      "2017-06-12 00:19:17 - Epoch 23, CIFAR-10 Batch 1:  Training Loss: 25047.0508 Trainning Accuracy: 0.525000 Validation Accuracy: 0.297400\n",
      "2017-06-12 00:21:14 - Epoch 24, CIFAR-10 Batch 1:  Training Loss: 14984.2344 Trainning Accuracy: 0.500000 Validation Accuracy: 0.284200\n",
      "2017-06-12 00:23:13 - Epoch 25, CIFAR-10 Batch 1:  Training Loss: 19154.5781 Trainning Accuracy: 0.475000 Validation Accuracy: 0.265800\n",
      "2017-06-12 00:25:12 - Epoch 26, CIFAR-10 Batch 1:  Training Loss: 14917.3066 Trainning Accuracy: 0.525000 Validation Accuracy: 0.245800\n",
      "2017-06-12 00:27:10 - Epoch 27, CIFAR-10 Batch 1:  Training Loss:  9161.2188 Trainning Accuracy: 0.475000 Validation Accuracy: 0.239000\n",
      "2017-06-12 00:29:07 - Epoch 28, CIFAR-10 Batch 1:  Training Loss:  6803.3984 Trainning Accuracy: 0.400000 Validation Accuracy: 0.227000\n",
      "2017-06-12 00:31:04 - Epoch 29, CIFAR-10 Batch 1:  Training Loss:  3313.9500 Trainning Accuracy: 0.400000 Validation Accuracy: 0.218400\n",
      "2017-06-12 00:33:02 - Epoch 30, CIFAR-10 Batch 1:  Training Loss:  1435.5081 Trainning Accuracy: 0.475000 Validation Accuracy: 0.200000\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 30\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .4 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 13:13:39 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 7007341.0000 Trainning Accuracy: 0.200000 Validation Accuracy: 0.313400\n",
      "2017-06-10 13:15:36 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 3972988.0000 Trainning Accuracy: 0.300000 Validation Accuracy: 0.361200\n",
      "2017-06-10 13:17:34 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 2633126.5000 Trainning Accuracy: 0.325000 Validation Accuracy: 0.386400\n",
      "2017-06-10 13:19:32 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 1736961.7500 Trainning Accuracy: 0.450000 Validation Accuracy: 0.403400\n",
      "2017-06-10 13:21:30 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 1370117.2500 Trainning Accuracy: 0.500000 Validation Accuracy: 0.413000\n",
      "2017-06-10 13:23:27 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 958361.1875 Trainning Accuracy: 0.475000 Validation Accuracy: 0.419800\n",
      "2017-06-10 13:25:24 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 728548.8750 Trainning Accuracy: 0.475000 Validation Accuracy: 0.418200\n",
      "2017-06-10 13:27:21 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 543312.3750 Trainning Accuracy: 0.550000 Validation Accuracy: 0.413600\n",
      "2017-06-10 13:29:18 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 499388.9062 Trainning Accuracy: 0.525000 Validation Accuracy: 0.410800\n",
      "2017-06-10 13:31:14 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 387972.6875 Trainning Accuracy: 0.525000 Validation Accuracy: 0.407000\n",
      "2017-06-10 13:33:11 - Epoch 11, CIFAR-10 Batch 1:  Training Loss: 311811.5625 Trainning Accuracy: 0.500000 Validation Accuracy: 0.400400\n",
      "2017-06-10 13:35:08 - Epoch 12, CIFAR-10 Batch 1:  Training Loss: 224950.2812 Trainning Accuracy: 0.575000 Validation Accuracy: 0.395800\n",
      "2017-06-10 13:37:04 - Epoch 13, CIFAR-10 Batch 1:  Training Loss: 186343.2969 Trainning Accuracy: 0.675000 Validation Accuracy: 0.395400\n",
      "2017-06-10 13:39:00 - Epoch 14, CIFAR-10 Batch 1:  Training Loss: 149008.0625 Trainning Accuracy: 0.650000 Validation Accuracy: 0.379600\n",
      "2017-06-10 13:40:57 - Epoch 15, CIFAR-10 Batch 1:  Training Loss: 129894.3906 Trainning Accuracy: 0.625000 Validation Accuracy: 0.375200\n",
      "2017-06-10 13:42:54 - Epoch 16, CIFAR-10 Batch 1:  Training Loss: 90129.2266 Trainning Accuracy: 0.650000 Validation Accuracy: 0.372600\n",
      "2017-06-10 13:44:50 - Epoch 17, CIFAR-10 Batch 1:  Training Loss: 74237.1484 Trainning Accuracy: 0.575000 Validation Accuracy: 0.368000\n",
      "2017-06-10 13:46:47 - Epoch 18, CIFAR-10 Batch 1:  Training Loss: 57151.5938 Trainning Accuracy: 0.675000 Validation Accuracy: 0.358200\n",
      "2017-06-10 13:48:43 - Epoch 19, CIFAR-10 Batch 1:  Training Loss: 45404.0117 Trainning Accuracy: 0.675000 Validation Accuracy: 0.357000\n",
      "2017-06-10 13:50:40 - Epoch 20, CIFAR-10 Batch 1:  Training Loss: 29959.0352 Trainning Accuracy: 0.625000 Validation Accuracy: 0.344000\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .33 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 13:52:40 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 30771622.0000 Trainning Accuracy: 0.175000 Validation Accuracy: 0.130800\n",
      "2017-06-10 13:54:37 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 11927043.0000 Trainning Accuracy: 0.175000 Validation Accuracy: 0.241400\n",
      "2017-06-10 13:56:34 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 6772763.0000 Trainning Accuracy: 0.350000 Validation Accuracy: 0.304400\n",
      "2017-06-10 13:58:30 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 4204565.5000 Trainning Accuracy: 0.425000 Validation Accuracy: 0.336000\n",
      "2017-06-10 14:00:27 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 3120414.0000 Trainning Accuracy: 0.475000 Validation Accuracy: 0.356000\n",
      "2017-06-10 14:02:24 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 2246949.2500 Trainning Accuracy: 0.475000 Validation Accuracy: 0.369600\n",
      "2017-06-10 14:04:21 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 1679961.7500 Trainning Accuracy: 0.500000 Validation Accuracy: 0.386600\n",
      "2017-06-10 14:06:18 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 1203273.6250 Trainning Accuracy: 0.600000 Validation Accuracy: 0.389800\n",
      "2017-06-10 14:08:14 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 866861.6250 Trainning Accuracy: 0.600000 Validation Accuracy: 0.401600\n",
      "2017-06-10 14:10:11 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 661689.1250 Trainning Accuracy: 0.600000 Validation Accuracy: 0.391000\n",
      "2017-06-10 14:12:08 - Epoch 11, CIFAR-10 Batch 1:  Training Loss: 525865.5000 Trainning Accuracy: 0.575000 Validation Accuracy: 0.384600\n",
      "2017-06-10 14:14:05 - Epoch 12, CIFAR-10 Batch 1:  Training Loss: 396837.9062 Trainning Accuracy: 0.550000 Validation Accuracy: 0.380400\n",
      "2017-06-10 14:16:02 - Epoch 13, CIFAR-10 Batch 1:  Training Loss: 240576.0938 Trainning Accuracy: 0.575000 Validation Accuracy: 0.375600\n",
      "2017-06-10 14:17:59 - Epoch 14, CIFAR-10 Batch 1:  Training Loss: 228902.2500 Trainning Accuracy: 0.525000 Validation Accuracy: 0.352800\n",
      "2017-06-10 14:19:56 - Epoch 15, CIFAR-10 Batch 1:  Training Loss: 155075.7188 Trainning Accuracy: 0.575000 Validation Accuracy: 0.336800\n",
      "2017-06-10 14:21:53 - Epoch 16, CIFAR-10 Batch 1:  Training Loss: 102018.2500 Trainning Accuracy: 0.550000 Validation Accuracy: 0.311400\n",
      "2017-06-10 14:23:49 - Epoch 17, CIFAR-10 Batch 1:  Training Loss: 81942.7344 Trainning Accuracy: 0.550000 Validation Accuracy: 0.289400\n",
      "2017-06-10 14:25:46 - Epoch 18, CIFAR-10 Batch 1:  Training Loss: 63684.7383 Trainning Accuracy: 0.500000 Validation Accuracy: 0.262400\n",
      "2017-06-10 14:27:43 - Epoch 19, CIFAR-10 Batch 1:  Training Loss: 55172.3516 Trainning Accuracy: 0.425000 Validation Accuracy: 0.235200\n",
      "2017-06-10 14:29:40 - Epoch 20, CIFAR-10 Batch 1:  Training Loss: 45161.6289 Trainning Accuracy: 0.425000 Validation Accuracy: 0.217800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .3 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# novo treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 16)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4096)\n",
      "(?, 5000) with dropout\n",
      "(?, 5000) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 16)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4096)\n",
      "(?, 5000) with dropout\n",
      "(?, 5000) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 16 , 'cvmp2': 64, 'cvmp3': 192}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (2, 2)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 5000, 'fc2' : 5000, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    #x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    #print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-11 16:59:24 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 2369141.7500 Trainning Accuracy: 0.175000 Validation Accuracy: 0.187200\n",
      "2017-06-11 17:01:20 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 1377758.0000 Trainning Accuracy: 0.250000 Validation Accuracy: 0.273400\n",
      "2017-06-11 17:03:09 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 810266.6250 Trainning Accuracy: 0.275000 Validation Accuracy: 0.305000\n",
      "2017-06-11 17:04:55 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 533106.6875 Trainning Accuracy: 0.350000 Validation Accuracy: 0.325800\n",
      "2017-06-11 17:06:42 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 370532.3750 Trainning Accuracy: 0.425000 Validation Accuracy: 0.343400\n",
      "2017-06-11 17:08:27 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 317470.8125 Trainning Accuracy: 0.450000 Validation Accuracy: 0.349000\n",
      "2017-06-11 17:10:13 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 242078.5000 Trainning Accuracy: 0.525000 Validation Accuracy: 0.359000\n",
      "2017-06-11 17:11:59 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 194448.1562 Trainning Accuracy: 0.525000 Validation Accuracy: 0.355600\n",
      "2017-06-11 17:13:49 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 163527.9688 Trainning Accuracy: 0.525000 Validation Accuracy: 0.364200\n",
      "2017-06-11 17:15:38 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 130080.9375 Trainning Accuracy: 0.550000 Validation Accuracy: 0.362400\n",
      "2017-06-11 17:17:27 - Epoch 11, CIFAR-10 Batch 1:  Training Loss: 105751.1016 Trainning Accuracy: 0.625000 Validation Accuracy: 0.374600\n",
      "2017-06-11 17:19:15 - Epoch 12, CIFAR-10 Batch 1:  Training Loss: 86339.7109 Trainning Accuracy: 0.575000 Validation Accuracy: 0.376800\n",
      "2017-06-11 17:21:05 - Epoch 13, CIFAR-10 Batch 1:  Training Loss: 63418.3750 Trainning Accuracy: 0.600000 Validation Accuracy: 0.371800\n",
      "2017-06-11 17:22:54 - Epoch 14, CIFAR-10 Batch 1:  Training Loss: 52504.1484 Trainning Accuracy: 0.625000 Validation Accuracy: 0.372000\n",
      "2017-06-11 17:24:44 - Epoch 15, CIFAR-10 Batch 1:  Training Loss: 40700.5234 Trainning Accuracy: 0.650000 Validation Accuracy: 0.367800\n",
      "2017-06-11 17:26:32 - Epoch 16, CIFAR-10 Batch 1:  Training Loss: 29916.2852 Trainning Accuracy: 0.625000 Validation Accuracy: 0.381200\n",
      "2017-06-11 17:28:21 - Epoch 17, CIFAR-10 Batch 1:  Training Loss: 29016.0938 Trainning Accuracy: 0.625000 Validation Accuracy: 0.375600\n",
      "2017-06-11 17:30:10 - Epoch 18, CIFAR-10 Batch 1:  Training Loss: 23243.8496 Trainning Accuracy: 0.625000 Validation Accuracy: 0.362800\n",
      "2017-06-11 17:31:59 - Epoch 19, CIFAR-10 Batch 1:  Training Loss: 14694.4971 Trainning Accuracy: 0.650000 Validation Accuracy: 0.356800\n",
      "2017-06-11 17:33:47 - Epoch 20, CIFAR-10 Batch 1:  Training Loss: 12764.7656 Trainning Accuracy: 0.600000 Validation Accuracy: 0.341200\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .3 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .5 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .7 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando 2 camadas conv 1 camada fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 12)\n",
      "(?, 8, 8, 48)\n",
      "(?, 3072)\n",
      "(?, 6144) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 12)\n",
      "(?, 8, 8, 48)\n",
      "(?, 3072)\n",
      "(?, 6144) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 12 , 'cvmp2': 48, 'cvmp3': 192}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (3, 3), 'cvmp3': (2, 2)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 3072*2, 'fc2' : 3072*2, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    #x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    #print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    #x = fully_conn(x, num_outputs['fc2'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 18:25:29 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 11678.2051 Trainning Accuracy: 0.300000 Validation Accuracy: 0.289200\n",
      "2017-06-10 18:26:19 - Epoch  2, CIFAR-10 Batch 1:  Training Loss:  7359.8975 Trainning Accuracy: 0.425000 Validation Accuracy: 0.341600\n",
      "2017-06-10 18:27:09 - Epoch  3, CIFAR-10 Batch 1:  Training Loss:  5143.0728 Trainning Accuracy: 0.475000 Validation Accuracy: 0.369000\n",
      "2017-06-10 18:28:00 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  3363.7539 Trainning Accuracy: 0.525000 Validation Accuracy: 0.388800\n",
      "2017-06-10 18:28:50 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  2139.7063 Trainning Accuracy: 0.525000 Validation Accuracy: 0.398600\n",
      "2017-06-10 18:29:41 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  1626.9167 Trainning Accuracy: 0.525000 Validation Accuracy: 0.402800\n",
      "2017-06-10 18:30:31 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  1034.4563 Trainning Accuracy: 0.550000 Validation Accuracy: 0.403200\n",
      "2017-06-10 18:31:21 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:   797.8840 Trainning Accuracy: 0.550000 Validation Accuracy: 0.404600\n",
      "2017-06-10 18:32:11 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:   685.5762 Trainning Accuracy: 0.600000 Validation Accuracy: 0.408600\n",
      "2017-06-10 18:33:00 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:   443.0820 Trainning Accuracy: 0.675000 Validation Accuracy: 0.410400\n",
      "2017-06-10 18:33:50 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:   318.7510 Trainning Accuracy: 0.675000 Validation Accuracy: 0.418400\n",
      "2017-06-10 18:34:40 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:   261.4155 Trainning Accuracy: 0.700000 Validation Accuracy: 0.417000\n",
      "2017-06-10 18:35:29 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:   169.5616 Trainning Accuracy: 0.725000 Validation Accuracy: 0.415400\n",
      "2017-06-10 18:36:19 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:   119.9107 Trainning Accuracy: 0.775000 Validation Accuracy: 0.413000\n",
      "2017-06-10 18:37:08 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:    74.6630 Trainning Accuracy: 0.800000 Validation Accuracy: 0.414600\n",
      "2017-06-10 18:37:58 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:    37.6481 Trainning Accuracy: 0.900000 Validation Accuracy: 0.410800\n",
      "2017-06-10 18:38:47 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:    52.2706 Trainning Accuracy: 0.875000 Validation Accuracy: 0.412400\n",
      "2017-06-10 18:39:37 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:    38.6626 Trainning Accuracy: 0.900000 Validation Accuracy: 0.413200\n",
      "2017-06-10 18:40:27 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:    32.3527 Trainning Accuracy: 0.875000 Validation Accuracy: 0.410400\n",
      "2017-06-10 18:41:16 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:    23.3580 Trainning Accuracy: 0.850000 Validation Accuracy: 0.408400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .3 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 18:42:07 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 25029.4258 Trainning Accuracy: 0.125000 Validation Accuracy: 0.266000\n",
      "2017-06-10 18:42:56 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 17139.4648 Trainning Accuracy: 0.275000 Validation Accuracy: 0.329000\n",
      "2017-06-10 18:43:45 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 10110.6396 Trainning Accuracy: 0.350000 Validation Accuracy: 0.376800\n",
      "2017-06-10 18:44:36 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  6532.5151 Trainning Accuracy: 0.525000 Validation Accuracy: 0.390600\n",
      "2017-06-10 18:45:25 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  4381.8623 Trainning Accuracy: 0.475000 Validation Accuracy: 0.410200\n",
      "2017-06-10 18:46:15 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  3339.3933 Trainning Accuracy: 0.575000 Validation Accuracy: 0.414000\n",
      "2017-06-10 18:47:05 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  2164.9087 Trainning Accuracy: 0.675000 Validation Accuracy: 0.418000\n",
      "2017-06-10 18:47:54 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:   984.3013 Trainning Accuracy: 0.750000 Validation Accuracy: 0.425200\n",
      "2017-06-10 18:48:44 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:   805.7221 Trainning Accuracy: 0.775000 Validation Accuracy: 0.428400\n",
      "2017-06-10 18:49:34 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:   434.2703 Trainning Accuracy: 0.800000 Validation Accuracy: 0.431400\n",
      "2017-06-10 18:50:24 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:   279.7522 Trainning Accuracy: 0.800000 Validation Accuracy: 0.435400\n",
      "2017-06-10 18:51:13 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:   126.6396 Trainning Accuracy: 0.825000 Validation Accuracy: 0.443400\n",
      "2017-06-10 18:52:03 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:   118.1519 Trainning Accuracy: 0.825000 Validation Accuracy: 0.440000\n",
      "2017-06-10 18:52:53 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:   110.2512 Trainning Accuracy: 0.825000 Validation Accuracy: 0.441400\n",
      "2017-06-10 18:53:42 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:    51.1356 Trainning Accuracy: 0.875000 Validation Accuracy: 0.439400\n",
      "2017-06-10 18:54:33 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:    80.4481 Trainning Accuracy: 0.900000 Validation Accuracy: 0.431600\n",
      "2017-06-10 18:55:23 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:    61.2885 Trainning Accuracy: 0.900000 Validation Accuracy: 0.425800\n",
      "2017-06-10 18:56:12 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:    43.4148 Trainning Accuracy: 0.925000 Validation Accuracy: 0.423600\n",
      "2017-06-10 18:57:02 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:    23.6851 Trainning Accuracy: 0.875000 Validation Accuracy: 0.419800\n",
      "2017-06-10 18:57:52 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:    19.5761 Trainning Accuracy: 0.925000 Validation Accuracy: 0.421800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .33 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 18:58:43 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 24136.8594 Trainning Accuracy: 0.300000 Validation Accuracy: 0.291000\n",
      "2017-06-10 18:59:32 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 15998.0781 Trainning Accuracy: 0.425000 Validation Accuracy: 0.345400\n",
      "2017-06-10 19:00:22 - Epoch  3, CIFAR-10 Batch 1:  Training Loss:  9089.4209 Trainning Accuracy: 0.475000 Validation Accuracy: 0.386400\n",
      "2017-06-10 19:01:12 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  6228.0479 Trainning Accuracy: 0.525000 Validation Accuracy: 0.395600\n",
      "2017-06-10 19:02:02 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  4651.6060 Trainning Accuracy: 0.575000 Validation Accuracy: 0.415800\n",
      "2017-06-10 19:02:52 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  3450.4126 Trainning Accuracy: 0.650000 Validation Accuracy: 0.430200\n",
      "2017-06-10 19:03:42 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  2415.9180 Trainning Accuracy: 0.675000 Validation Accuracy: 0.427800\n",
      "2017-06-10 19:04:32 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  1740.4529 Trainning Accuracy: 0.725000 Validation Accuracy: 0.439400\n",
      "2017-06-10 19:05:23 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:  1383.5430 Trainning Accuracy: 0.800000 Validation Accuracy: 0.451400\n",
      "2017-06-10 19:06:13 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:   955.6172 Trainning Accuracy: 0.800000 Validation Accuracy: 0.447400\n",
      "2017-06-10 19:07:03 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:   775.0800 Trainning Accuracy: 0.825000 Validation Accuracy: 0.450600\n",
      "2017-06-10 19:07:53 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:   631.8228 Trainning Accuracy: 0.875000 Validation Accuracy: 0.450400\n",
      "2017-06-10 19:08:43 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:   549.0045 Trainning Accuracy: 0.875000 Validation Accuracy: 0.449800\n",
      "2017-06-10 19:09:33 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:   426.5595 Trainning Accuracy: 0.850000 Validation Accuracy: 0.447200\n",
      "2017-06-10 19:10:24 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:   327.0386 Trainning Accuracy: 0.875000 Validation Accuracy: 0.449600\n",
      "2017-06-10 19:11:14 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:   218.0284 Trainning Accuracy: 0.900000 Validation Accuracy: 0.446200\n",
      "2017-06-10 19:12:03 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:   133.1198 Trainning Accuracy: 0.925000 Validation Accuracy: 0.455400\n",
      "2017-06-10 19:12:53 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:    89.8837 Trainning Accuracy: 0.950000 Validation Accuracy: 0.448400\n",
      "2017-06-10 19:13:43 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:    78.8568 Trainning Accuracy: 0.950000 Validation Accuracy: 0.451400\n",
      "2017-06-10 19:14:34 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:    86.7134 Trainning Accuracy: 0.950000 Validation Accuracy: 0.444000\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .4 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 19:15:25 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 32191.3750 Trainning Accuracy: 0.200000 Validation Accuracy: 0.245400\n",
      "2017-06-10 19:16:15 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 18992.1777 Trainning Accuracy: 0.350000 Validation Accuracy: 0.328400\n",
      "2017-06-10 19:17:05 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 11789.0703 Trainning Accuracy: 0.450000 Validation Accuracy: 0.374000\n",
      "2017-06-10 19:17:55 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  7836.3955 Trainning Accuracy: 0.500000 Validation Accuracy: 0.392600\n",
      "2017-06-10 19:18:45 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  5421.7578 Trainning Accuracy: 0.600000 Validation Accuracy: 0.406200\n",
      "2017-06-10 19:19:35 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  3631.3992 Trainning Accuracy: 0.625000 Validation Accuracy: 0.416200\n",
      "2017-06-10 19:20:25 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  2455.7725 Trainning Accuracy: 0.650000 Validation Accuracy: 0.409800\n",
      "2017-06-10 19:21:14 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  1434.7241 Trainning Accuracy: 0.725000 Validation Accuracy: 0.423600\n",
      "2017-06-10 19:22:04 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:   940.6546 Trainning Accuracy: 0.825000 Validation Accuracy: 0.424800\n",
      "2017-06-10 19:22:54 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:   748.4034 Trainning Accuracy: 0.825000 Validation Accuracy: 0.428200\n",
      "2017-06-10 19:23:44 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:   782.7379 Trainning Accuracy: 0.825000 Validation Accuracy: 0.432000\n",
      "2017-06-10 19:24:35 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:   537.4648 Trainning Accuracy: 0.825000 Validation Accuracy: 0.435800\n",
      "2017-06-10 19:25:25 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:   447.6132 Trainning Accuracy: 0.800000 Validation Accuracy: 0.435400\n",
      "2017-06-10 19:26:15 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:   344.4730 Trainning Accuracy: 0.850000 Validation Accuracy: 0.434400\n",
      "2017-06-10 19:27:05 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:   134.2247 Trainning Accuracy: 0.925000 Validation Accuracy: 0.439400\n",
      "2017-06-10 19:27:55 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:   139.4154 Trainning Accuracy: 0.925000 Validation Accuracy: 0.441400\n",
      "2017-06-10 19:28:45 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:    55.9155 Trainning Accuracy: 0.950000 Validation Accuracy: 0.439600\n",
      "2017-06-10 19:29:35 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:    15.7770 Trainning Accuracy: 0.950000 Validation Accuracy: 0.443800\n",
      "2017-06-10 19:30:25 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:    11.5454 Trainning Accuracy: 0.975000 Validation Accuracy: 0.442600\n",
      "2017-06-10 19:31:15 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:    38.9486 Trainning Accuracy: 0.950000 Validation Accuracy: 0.439800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .5 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 19:32:06 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 16875.3477 Trainning Accuracy: 0.250000 Validation Accuracy: 0.297400\n",
      "2017-06-10 19:32:56 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 11813.9395 Trainning Accuracy: 0.350000 Validation Accuracy: 0.348200\n",
      "2017-06-10 19:33:46 - Epoch  3, CIFAR-10 Batch 1:  Training Loss:  6624.9634 Trainning Accuracy: 0.475000 Validation Accuracy: 0.379200\n",
      "2017-06-10 19:34:36 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  3777.4902 Trainning Accuracy: 0.500000 Validation Accuracy: 0.401000\n",
      "2017-06-10 19:35:27 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  2643.3989 Trainning Accuracy: 0.575000 Validation Accuracy: 0.405000\n",
      "2017-06-10 19:36:17 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  1665.7585 Trainning Accuracy: 0.650000 Validation Accuracy: 0.424400\n",
      "2017-06-10 19:37:07 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  1234.0553 Trainning Accuracy: 0.750000 Validation Accuracy: 0.427600\n",
      "2017-06-10 19:37:57 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:   770.2678 Trainning Accuracy: 0.725000 Validation Accuracy: 0.432000\n",
      "2017-06-10 19:38:47 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:   417.5775 Trainning Accuracy: 0.825000 Validation Accuracy: 0.436400\n",
      "2017-06-10 19:39:37 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:   222.1101 Trainning Accuracy: 0.875000 Validation Accuracy: 0.431600\n",
      "2017-06-10 19:40:27 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:     8.6802 Trainning Accuracy: 0.925000 Validation Accuracy: 0.438200\n",
      "2017-06-10 19:41:17 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:    19.3686 Trainning Accuracy: 0.925000 Validation Accuracy: 0.438400\n",
      "2017-06-10 19:42:07 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:     6.7785 Trainning Accuracy: 0.975000 Validation Accuracy: 0.449800\n",
      "2017-06-10 19:42:57 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.448800\n",
      "2017-06-10 19:43:47 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:     6.4653 Trainning Accuracy: 0.975000 Validation Accuracy: 0.452800\n",
      "2017-06-10 19:44:38 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.461600\n",
      "2017-06-10 19:45:28 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.455200\n",
      "2017-06-10 19:46:18 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.465400\n",
      "2017-06-10 19:47:08 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.467800\n",
      "2017-06-10 19:47:58 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.470800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .6 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 19:48:49 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 27581.1504 Trainning Accuracy: 0.200000 Validation Accuracy: 0.289400\n",
      "2017-06-10 19:49:39 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 15932.2266 Trainning Accuracy: 0.350000 Validation Accuracy: 0.339400\n",
      "2017-06-10 19:50:29 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 12762.5400 Trainning Accuracy: 0.375000 Validation Accuracy: 0.353600\n",
      "2017-06-10 19:51:19 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  7471.0498 Trainning Accuracy: 0.500000 Validation Accuracy: 0.397800\n",
      "2017-06-10 19:52:09 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  3318.4937 Trainning Accuracy: 0.650000 Validation Accuracy: 0.415600\n",
      "2017-06-10 19:52:58 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  2232.4568 Trainning Accuracy: 0.725000 Validation Accuracy: 0.416000\n",
      "2017-06-10 19:53:48 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  1531.5457 Trainning Accuracy: 0.775000 Validation Accuracy: 0.422800\n",
      "2017-06-10 19:54:39 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  1225.1233 Trainning Accuracy: 0.825000 Validation Accuracy: 0.437200\n",
      "2017-06-10 19:55:29 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:   959.5082 Trainning Accuracy: 0.850000 Validation Accuracy: 0.434400\n",
      "2017-06-10 19:56:19 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:   673.7448 Trainning Accuracy: 0.850000 Validation Accuracy: 0.435600\n",
      "2017-06-10 19:57:09 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:   441.2067 Trainning Accuracy: 0.875000 Validation Accuracy: 0.437400\n",
      "2017-06-10 19:57:59 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:   277.7104 Trainning Accuracy: 0.900000 Validation Accuracy: 0.446200\n",
      "2017-06-10 19:58:48 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:   220.1595 Trainning Accuracy: 0.925000 Validation Accuracy: 0.450400\n",
      "2017-06-10 19:59:39 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:    85.5491 Trainning Accuracy: 0.925000 Validation Accuracy: 0.448400\n",
      "2017-06-10 20:00:29 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:    42.5449 Trainning Accuracy: 0.975000 Validation Accuracy: 0.456600\n",
      "2017-06-10 20:01:18 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:    41.9914 Trainning Accuracy: 0.950000 Validation Accuracy: 0.464000\n",
      "2017-06-10 20:02:09 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:     4.9547 Trainning Accuracy: 0.975000 Validation Accuracy: 0.462200\n",
      "2017-06-10 20:02:59 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:    14.9557 Trainning Accuracy: 0.975000 Validation Accuracy: 0.466400\n",
      "2017-06-10 20:03:49 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.459400\n",
      "2017-06-10 20:04:39 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.461600\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .7 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-10 20:05:30 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 17203.9727 Trainning Accuracy: 0.200000 Validation Accuracy: 0.272400\n",
      "2017-06-10 20:06:20 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 11268.8457 Trainning Accuracy: 0.350000 Validation Accuracy: 0.350800\n",
      "2017-06-10 20:07:10 - Epoch  3, CIFAR-10 Batch 1:  Training Loss:  6809.5781 Trainning Accuracy: 0.500000 Validation Accuracy: 0.378400\n",
      "2017-06-10 20:08:00 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  4728.5386 Trainning Accuracy: 0.450000 Validation Accuracy: 0.385800\n",
      "2017-06-10 20:08:50 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  3044.5679 Trainning Accuracy: 0.575000 Validation Accuracy: 0.407200\n",
      "2017-06-10 20:09:40 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  2039.4238 Trainning Accuracy: 0.600000 Validation Accuracy: 0.415600\n",
      "2017-06-10 20:10:30 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  1415.7285 Trainning Accuracy: 0.725000 Validation Accuracy: 0.416000\n",
      "2017-06-10 20:11:20 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:   914.7025 Trainning Accuracy: 0.775000 Validation Accuracy: 0.423800\n",
      "2017-06-10 20:12:10 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:   461.4458 Trainning Accuracy: 0.825000 Validation Accuracy: 0.423400\n",
      "2017-06-10 20:13:00 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:   356.1221 Trainning Accuracy: 0.875000 Validation Accuracy: 0.425000\n",
      "2017-06-10 20:13:49 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:   279.5331 Trainning Accuracy: 0.925000 Validation Accuracy: 0.420200\n",
      "2017-06-10 20:14:40 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:   223.1710 Trainning Accuracy: 0.875000 Validation Accuracy: 0.430400\n",
      "2017-06-10 20:15:29 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:   142.7235 Trainning Accuracy: 0.925000 Validation Accuracy: 0.434600\n",
      "2017-06-10 20:16:20 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:    51.7991 Trainning Accuracy: 0.975000 Validation Accuracy: 0.444200\n",
      "2017-06-10 20:17:10 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:    17.1688 Trainning Accuracy: 0.975000 Validation Accuracy: 0.443000\n",
      "2017-06-10 20:17:59 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:     1.7034 Trainning Accuracy: 0.975000 Validation Accuracy: 0.440600\n",
      "2017-06-10 20:18:50 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.447400\n",
      "2017-06-10 20:19:40 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.441200\n",
      "2017-06-10 20:20:30 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.446400\n",
      "2017-06-10 20:21:20 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:     0.0000 Trainning Accuracy: 1.000000 Validation Accuracy: 0.443200\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 256 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests after reading tensorflow tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 2048) with dropout\n",
      "(?, 2048) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 2048) with dropout\n",
      "(?, 2048) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*2, 'fc2' : 1024*2, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-15 12:30:51 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 103130.2266 Trainning Accuracy: 0.200000 Validation Accuracy: 0.270800\n",
      "2017-06-15 12:33:08 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 35653.1211 Trainning Accuracy: 0.375000 Validation Accuracy: 0.318400\n",
      "2017-06-15 12:35:24 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 22412.9727 Trainning Accuracy: 0.425000 Validation Accuracy: 0.300800\n",
      "2017-06-15 12:37:40 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 16072.0654 Trainning Accuracy: 0.425000 Validation Accuracy: 0.339000\n",
      "2017-06-15 12:39:55 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  5827.7261 Trainning Accuracy: 0.375000 Validation Accuracy: 0.285000\n",
      "2017-06-15 12:42:12 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  1607.7531 Trainning Accuracy: 0.200000 Validation Accuracy: 0.185000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-c94978c33d58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-81050f085b79>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# DONE: Implement Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \"\"\"\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 3072) with dropout\n",
      "(?, 3072) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 3072) with dropout\n",
      "(?, 3072) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*3, 'fc2' : 1024*3, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-15 12:44:59 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 188998.1406 Trainning Accuracy: 0.275000 Validation Accuracy: 0.280600\n",
      "2017-06-15 12:47:27 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 76063.3828 Trainning Accuracy: 0.350000 Validation Accuracy: 0.332200\n",
      "2017-06-15 12:49:56 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 43382.4102 Trainning Accuracy: 0.350000 Validation Accuracy: 0.336800\n",
      "2017-06-15 12:52:24 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 27793.1055 Trainning Accuracy: 0.350000 Validation Accuracy: 0.328600\n",
      "2017-06-15 12:54:58 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 15951.1377 Trainning Accuracy: 0.425000 Validation Accuracy: 0.357400\n",
      "2017-06-15 12:57:33 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  9387.7568 Trainning Accuracy: 0.425000 Validation Accuracy: 0.330000\n",
      "2017-06-15 13:00:01 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  2089.9834 Trainning Accuracy: 0.425000 Validation Accuracy: 0.277400\n",
      "2017-06-15 13:02:29 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:   832.8766 Trainning Accuracy: 0.275000 Validation Accuracy: 0.177800\n",
      "2017-06-15 13:04:58 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:   598.5699 Trainning Accuracy: 0.200000 Validation Accuracy: 0.154400\n",
      "2017-06-15 13:07:26 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:   207.1432 Trainning Accuracy: 0.200000 Validation Accuracy: 0.133800\n",
      "2017-06-15 13:09:54 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:    60.5376 Trainning Accuracy: 0.200000 Validation Accuracy: 0.133400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c94978c33d58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-81050f085b79>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# DONE: Implement Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \"\"\"\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 4096) with dropout\n",
      "(?, 4096) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 4096) with dropout\n",
      "(?, 4096) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*4, 'fc2' : 1024*4, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-15 13:14:39 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 298196.6875 Trainning Accuracy: 0.300000 Validation Accuracy: 0.283400\n",
      "2017-06-15 13:17:24 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 129758.5000 Trainning Accuracy: 0.400000 Validation Accuracy: 0.350200\n",
      "2017-06-15 13:20:08 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 90056.9219 Trainning Accuracy: 0.400000 Validation Accuracy: 0.337000\n",
      "2017-06-15 13:22:52 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 48654.3281 Trainning Accuracy: 0.375000 Validation Accuracy: 0.377000\n",
      "2017-06-15 13:25:36 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 35819.1562 Trainning Accuracy: 0.500000 Validation Accuracy: 0.364800\n",
      "2017-06-15 13:28:19 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 20281.1367 Trainning Accuracy: 0.525000 Validation Accuracy: 0.404000\n",
      "2017-06-15 13:31:01 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 14938.5938 Trainning Accuracy: 0.600000 Validation Accuracy: 0.390400\n",
      "2017-06-15 13:33:43 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  9224.6689 Trainning Accuracy: 0.575000 Validation Accuracy: 0.383400\n",
      "2017-06-15 13:36:25 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:  5340.0029 Trainning Accuracy: 0.625000 Validation Accuracy: 0.356200\n",
      "2017-06-15 13:39:08 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:  2631.6694 Trainning Accuracy: 0.350000 Validation Accuracy: 0.263600\n",
      "2017-06-15 13:41:50 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:   838.8324 Trainning Accuracy: 0.300000 Validation Accuracy: 0.186400\n",
      "2017-06-15 13:44:36 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:   354.3845 Trainning Accuracy: 0.275000 Validation Accuracy: 0.171400\n",
      "2017-06-15 13:47:21 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:   200.5558 Trainning Accuracy: 0.275000 Validation Accuracy: 0.156400\n",
      "2017-06-15 13:50:06 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:   142.5028 Trainning Accuracy: 0.225000 Validation Accuracy: 0.146200\n",
      "2017-06-15 13:52:50 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:    47.3140 Trainning Accuracy: 0.225000 Validation Accuracy: 0.135600\n",
      "2017-06-15 13:55:35 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:    26.4958 Trainning Accuracy: 0.225000 Validation Accuracy: 0.131600\n",
      "2017-06-15 13:58:19 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:     2.6719 Trainning Accuracy: 0.275000 Validation Accuracy: 0.120800\n",
      "2017-06-15 14:01:04 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:    27.3645 Trainning Accuracy: 0.200000 Validation Accuracy: 0.126600\n",
      "2017-06-15 14:03:49 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     8.8320 Trainning Accuracy: 0.200000 Validation Accuracy: 0.118600\n",
      "2017-06-15 14:06:34 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:    55.4517 Trainning Accuracy: 0.175000 Validation Accuracy: 0.121400\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 4096) with dropout\n",
      "(?, 4096) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 4096) with dropout\n",
      "(?, 4096) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (3, 3)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*4, 'fc2' : 1024*4, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-15 14:09:36 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 202636.4219 Trainning Accuracy: 0.325000 Validation Accuracy: 0.297800\n",
      "2017-06-15 14:12:39 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 94970.5000 Trainning Accuracy: 0.475000 Validation Accuracy: 0.359000\n",
      "2017-06-15 14:15:43 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 76355.7969 Trainning Accuracy: 0.350000 Validation Accuracy: 0.347400\n",
      "2017-06-15 14:18:47 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 48478.6836 Trainning Accuracy: 0.475000 Validation Accuracy: 0.388200\n",
      "2017-06-15 14:21:52 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 25234.6035 Trainning Accuracy: 0.550000 Validation Accuracy: 0.383400\n",
      "2017-06-15 14:24:56 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 18317.6562 Trainning Accuracy: 0.475000 Validation Accuracy: 0.379400\n",
      "2017-06-15 14:28:00 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 13091.8213 Trainning Accuracy: 0.550000 Validation Accuracy: 0.372200\n",
      "2017-06-15 14:31:04 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  9479.5215 Trainning Accuracy: 0.525000 Validation Accuracy: 0.359400\n",
      "2017-06-15 14:34:08 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:  6539.2080 Trainning Accuracy: 0.525000 Validation Accuracy: 0.373400\n",
      "2017-06-15 14:37:12 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:  3492.4055 Trainning Accuracy: 0.425000 Validation Accuracy: 0.317000\n",
      "2017-06-15 14:40:16 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:  1404.1042 Trainning Accuracy: 0.275000 Validation Accuracy: 0.204400\n",
      "2017-06-15 14:43:23 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:   327.3441 Trainning Accuracy: 0.225000 Validation Accuracy: 0.157600\n",
      "2017-06-15 14:46:30 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:   120.8985 Trainning Accuracy: 0.300000 Validation Accuracy: 0.157200\n",
      "2017-06-15 14:49:38 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:    77.9705 Trainning Accuracy: 0.250000 Validation Accuracy: 0.142800\n",
      "2017-06-15 14:52:44 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:    98.1067 Trainning Accuracy: 0.175000 Validation Accuracy: 0.129000\n",
      "2017-06-15 14:55:53 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:    38.7117 Trainning Accuracy: 0.175000 Validation Accuracy: 0.130200\n",
      "2017-06-15 14:59:00 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:    43.2137 Trainning Accuracy: 0.150000 Validation Accuracy: 0.111000\n",
      "2017-06-15 15:02:07 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:    32.3459 Trainning Accuracy: 0.125000 Validation Accuracy: 0.122200\n",
      "2017-06-15 15:05:14 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:    17.0931 Trainning Accuracy: 0.150000 Validation Accuracy: 0.115600\n",
      "2017-06-15 15:08:21 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:    19.8516 Trainning Accuracy: 0.125000 Validation Accuracy: 0.114000\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 4096) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 4096) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (3, 3)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*4, 'fc2' : 1024*4, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    #x = fully_conn(x, num_outputs['fc2'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-15 15:52:18 - Epoch  1, CIFAR-10 Batch 1:  Training Loss:  5024.2891 Trainning Accuracy: 0.250000 Validation Accuracy: 0.279200\n",
      "2017-06-15 15:54:59 - Epoch  2, CIFAR-10 Batch 1:  Training Loss:  3710.5415 Trainning Accuracy: 0.225000 Validation Accuracy: 0.303600\n",
      "2017-06-15 15:57:42 - Epoch  3, CIFAR-10 Batch 1:  Training Loss:  2192.6775 Trainning Accuracy: 0.375000 Validation Accuracy: 0.343600\n",
      "2017-06-15 16:00:26 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  1786.2611 Trainning Accuracy: 0.275000 Validation Accuracy: 0.371200\n",
      "2017-06-15 16:03:04 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  1190.2750 Trainning Accuracy: 0.400000 Validation Accuracy: 0.375600\n",
      "2017-06-15 16:05:40 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:   776.3177 Trainning Accuracy: 0.525000 Validation Accuracy: 0.366600\n",
      "2017-06-15 16:08:16 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:   338.6270 Trainning Accuracy: 0.525000 Validation Accuracy: 0.361800\n",
      "2017-06-15 16:10:54 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:    96.9099 Trainning Accuracy: 0.450000 Validation Accuracy: 0.351600\n",
      "2017-06-15 16:13:32 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:    29.5003 Trainning Accuracy: 0.475000 Validation Accuracy: 0.249400\n",
      "2017-06-15 16:16:09 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:    16.3213 Trainning Accuracy: 0.325000 Validation Accuracy: 0.192800\n",
      "2017-06-15 16:18:45 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:    11.7821 Trainning Accuracy: 0.325000 Validation Accuracy: 0.166800\n",
      "2017-06-15 16:21:21 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:     8.3778 Trainning Accuracy: 0.250000 Validation Accuracy: 0.160800\n",
      "2017-06-15 16:23:57 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:     3.2160 Trainning Accuracy: 0.250000 Validation Accuracy: 0.139000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-c94978c33d58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-81050f085b79>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# DONE: Implement Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \"\"\"\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 8192) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 8192) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (3, 3)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*8, 'fc2' : 1024*8, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    #x = fully_conn(x, num_outputs['fc2'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-15 16:29:08 - Epoch  1, CIFAR-10 Batch 1:  Training Loss:  8250.3721 Trainning Accuracy: 0.375000 Validation Accuracy: 0.301200\n",
      "2017-06-15 16:31:58 - Epoch  2, CIFAR-10 Batch 1:  Training Loss:  8624.4746 Trainning Accuracy: 0.350000 Validation Accuracy: 0.302800\n",
      "2017-06-15 16:34:48 - Epoch  3, CIFAR-10 Batch 1:  Training Loss:  4633.6152 Trainning Accuracy: 0.375000 Validation Accuracy: 0.375800\n",
      "2017-06-15 16:37:51 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  5432.2632 Trainning Accuracy: 0.325000 Validation Accuracy: 0.318200\n",
      "2017-06-15 16:40:54 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  3165.2131 Trainning Accuracy: 0.575000 Validation Accuracy: 0.371600\n",
      "2017-06-15 16:43:58 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  2757.2380 Trainning Accuracy: 0.475000 Validation Accuracy: 0.392400\n",
      "2017-06-15 16:46:59 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  1993.9922 Trainning Accuracy: 0.550000 Validation Accuracy: 0.451400\n",
      "2017-06-15 16:49:51 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  1558.8367 Trainning Accuracy: 0.525000 Validation Accuracy: 0.412800\n",
      "2017-06-15 16:52:43 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:  1085.5485 Trainning Accuracy: 0.600000 Validation Accuracy: 0.440000\n",
      "2017-06-15 16:55:35 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:   958.5944 Trainning Accuracy: 0.475000 Validation Accuracy: 0.420800\n",
      "2017-06-15 16:58:28 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:   663.3823 Trainning Accuracy: 0.600000 Validation Accuracy: 0.436400\n",
      "2017-06-15 17:01:23 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:   352.6546 Trainning Accuracy: 0.625000 Validation Accuracy: 0.436800\n",
      "2017-06-15 17:04:18 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:   264.9694 Trainning Accuracy: 0.625000 Validation Accuracy: 0.384200\n",
      "2017-06-15 17:07:13 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:   102.4833 Trainning Accuracy: 0.550000 Validation Accuracy: 0.383400\n",
      "2017-06-15 17:10:11 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:    80.1948 Trainning Accuracy: 0.500000 Validation Accuracy: 0.357600\n",
      "2017-06-15 17:13:09 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:    52.5757 Trainning Accuracy: 0.450000 Validation Accuracy: 0.320000\n",
      "2017-06-15 17:16:07 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:    27.5005 Trainning Accuracy: 0.450000 Validation Accuracy: 0.290400\n",
      "2017-06-15 17:19:04 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:     9.8327 Trainning Accuracy: 0.450000 Validation Accuracy: 0.255600\n",
      "2017-06-15 17:22:00 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     7.3086 Trainning Accuracy: 0.425000 Validation Accuracy: 0.239800\n",
      "2017-06-15 17:24:55 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:     7.0035 Trainning Accuracy: 0.275000 Validation Accuracy: 0.228600\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 16384) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 16384) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (3, 3)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*16, 'fc2' : 1024*16, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    #x = fully_conn(x, num_outputs['fc2'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-15 17:28:16 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 11699.4346 Trainning Accuracy: 0.325000 Validation Accuracy: 0.288200\n",
      "2017-06-15 17:31:43 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 10681.3936 Trainning Accuracy: 0.300000 Validation Accuracy: 0.298200\n",
      "2017-06-15 17:35:20 - Epoch  3, CIFAR-10 Batch 1:  Training Loss:  6315.2720 Trainning Accuracy: 0.425000 Validation Accuracy: 0.384200\n",
      "2017-06-15 17:39:07 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  6378.4717 Trainning Accuracy: 0.425000 Validation Accuracy: 0.360600\n",
      "2017-06-15 17:43:04 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  4888.8545 Trainning Accuracy: 0.500000 Validation Accuracy: 0.399000\n",
      "2017-06-15 17:46:43 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  4724.3135 Trainning Accuracy: 0.500000 Validation Accuracy: 0.422600\n",
      "2017-06-15 17:50:09 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  3414.0398 Trainning Accuracy: 0.525000 Validation Accuracy: 0.430000\n",
      "2017-06-15 17:53:36 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  3483.1328 Trainning Accuracy: 0.600000 Validation Accuracy: 0.428400\n",
      "2017-06-15 17:57:01 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:  2793.3406 Trainning Accuracy: 0.625000 Validation Accuracy: 0.456400\n",
      "2017-06-15 18:00:28 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:  2263.5154 Trainning Accuracy: 0.600000 Validation Accuracy: 0.456000\n",
      "2017-06-15 18:04:00 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:  2103.9946 Trainning Accuracy: 0.700000 Validation Accuracy: 0.450600\n",
      "2017-06-15 18:07:30 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:  1674.3959 Trainning Accuracy: 0.575000 Validation Accuracy: 0.462600\n",
      "2017-06-15 18:10:59 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:  1033.9285 Trainning Accuracy: 0.700000 Validation Accuracy: 0.506200\n",
      "2017-06-15 18:14:27 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:  1067.5013 Trainning Accuracy: 0.625000 Validation Accuracy: 0.466400\n",
      "2017-06-15 18:17:56 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:   798.0250 Trainning Accuracy: 0.725000 Validation Accuracy: 0.479200\n",
      "2017-06-15 18:21:25 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:   569.3107 Trainning Accuracy: 0.750000 Validation Accuracy: 0.487400\n",
      "2017-06-15 18:24:56 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:   624.9780 Trainning Accuracy: 0.700000 Validation Accuracy: 0.446200\n",
      "2017-06-15 18:28:25 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:   672.8102 Trainning Accuracy: 0.625000 Validation Accuracy: 0.412800\n",
      "2017-06-15 18:31:55 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:   452.2848 Trainning Accuracy: 0.750000 Validation Accuracy: 0.468000\n",
      "2017-06-15 18:35:26 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:   398.2369 Trainning Accuracy: 0.700000 Validation Accuracy: 0.430000\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 24576) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 24576) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (3, 3)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*24, 'fc2' : 1024*24, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    #x = fully_conn(x, num_outputs['fc2'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-15 18:39:18 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 14862.3877 Trainning Accuracy: 0.250000 Validation Accuracy: 0.327200\n",
      "2017-06-15 18:43:13 - Epoch  2, CIFAR-10 Batch 1:  Training Loss:  8170.2939 Trainning Accuracy: 0.550000 Validation Accuracy: 0.390000\n",
      "2017-06-15 18:47:09 - Epoch  3, CIFAR-10 Batch 1:  Training Loss:  8789.9355 Trainning Accuracy: 0.450000 Validation Accuracy: 0.384600\n",
      "2017-06-15 18:51:05 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  6109.1914 Trainning Accuracy: 0.475000 Validation Accuracy: 0.388200\n",
      "2017-06-15 18:55:05 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  6463.3213 Trainning Accuracy: 0.550000 Validation Accuracy: 0.424200\n",
      "2017-06-15 18:58:59 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  5231.1689 Trainning Accuracy: 0.550000 Validation Accuracy: 0.405800\n",
      "2017-06-15 19:03:00 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  4052.6421 Trainning Accuracy: 0.550000 Validation Accuracy: 0.453200\n",
      "2017-06-15 19:06:55 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  5571.0947 Trainning Accuracy: 0.450000 Validation Accuracy: 0.390400\n",
      "2017-06-15 19:10:49 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:  4089.1360 Trainning Accuracy: 0.600000 Validation Accuracy: 0.461600\n",
      "2017-06-15 19:14:43 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:  3808.3679 Trainning Accuracy: 0.550000 Validation Accuracy: 0.436400\n",
      "2017-06-15 19:18:42 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:  2770.8831 Trainning Accuracy: 0.675000 Validation Accuracy: 0.468600\n",
      "2017-06-15 19:22:42 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:  4195.5913 Trainning Accuracy: 0.600000 Validation Accuracy: 0.419600\n",
      "2017-06-15 19:26:49 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:  2467.7981 Trainning Accuracy: 0.675000 Validation Accuracy: 0.464600\n",
      "2017-06-15 19:30:54 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:  1659.3724 Trainning Accuracy: 0.675000 Validation Accuracy: 0.451000\n",
      "2017-06-15 19:35:24 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:  1346.9480 Trainning Accuracy: 0.700000 Validation Accuracy: 0.484400\n",
      "2017-06-15 19:39:54 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:  1438.8650 Trainning Accuracy: 0.675000 Validation Accuracy: 0.497000\n",
      "2017-06-15 19:44:28 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:  1406.4583 Trainning Accuracy: 0.650000 Validation Accuracy: 0.471800\n",
      "2017-06-15 19:48:32 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:  1181.4906 Trainning Accuracy: 0.725000 Validation Accuracy: 0.482200\n",
      "2017-06-15 19:52:35 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:  1310.6741 Trainning Accuracy: 0.725000 Validation Accuracy: 0.478800\n",
      "2017-06-15 19:56:44 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:   464.2724 Trainning Accuracy: 0.775000 Validation Accuracy: 0.500800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 30720) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 30720) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (3, 3)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*30, 'fc2' : 1024*30, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    #x = fully_conn(x, num_outputs['fc2'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-16 08:10:36 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 13693.7314 Trainning Accuracy: 0.425000 Validation Accuracy: 0.319800\n",
      "2017-06-16 08:14:54 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 15926.9092 Trainning Accuracy: 0.400000 Validation Accuracy: 0.338400\n",
      "2017-06-16 08:19:14 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 11346.6758 Trainning Accuracy: 0.500000 Validation Accuracy: 0.393000\n",
      "2017-06-16 08:23:33 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  8352.9492 Trainning Accuracy: 0.450000 Validation Accuracy: 0.428800\n",
      "2017-06-16 08:27:51 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  8813.4590 Trainning Accuracy: 0.450000 Validation Accuracy: 0.392000\n",
      "2017-06-16 08:32:09 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  7417.6514 Trainning Accuracy: 0.525000 Validation Accuracy: 0.414400\n",
      "2017-06-16 08:36:28 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  6901.5898 Trainning Accuracy: 0.550000 Validation Accuracy: 0.430600\n",
      "2017-06-16 08:40:49 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  5476.5566 Trainning Accuracy: 0.575000 Validation Accuracy: 0.468600\n",
      "2017-06-16 08:45:09 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:  4022.3523 Trainning Accuracy: 0.675000 Validation Accuracy: 0.471000\n",
      "2017-06-16 08:49:27 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:  3224.4629 Trainning Accuracy: 0.675000 Validation Accuracy: 0.457600\n",
      "2017-06-16 08:53:47 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:  4402.1709 Trainning Accuracy: 0.575000 Validation Accuracy: 0.474400\n",
      "2017-06-16 08:58:11 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:  4196.7188 Trainning Accuracy: 0.600000 Validation Accuracy: 0.430600\n",
      "2017-06-16 09:02:36 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:  3500.5415 Trainning Accuracy: 0.600000 Validation Accuracy: 0.501400\n",
      "2017-06-16 09:07:01 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:  2209.2036 Trainning Accuracy: 0.775000 Validation Accuracy: 0.502000\n",
      "2017-06-16 09:11:26 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:  3046.4211 Trainning Accuracy: 0.675000 Validation Accuracy: 0.442600\n",
      "2017-06-16 09:15:52 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:  2289.4285 Trainning Accuracy: 0.625000 Validation Accuracy: 0.482600\n",
      "2017-06-16 09:20:18 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:  1907.3453 Trainning Accuracy: 0.675000 Validation Accuracy: 0.471400\n",
      "2017-06-16 09:24:46 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:  1199.0728 Trainning Accuracy: 0.775000 Validation Accuracy: 0.505000\n",
      "2017-06-16 09:29:13 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:  1191.5526 Trainning Accuracy: 0.775000 Validation Accuracy: 0.500800\n",
      "2017-06-16 09:33:41 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:  1681.6348 Trainning Accuracy: 0.725000 Validation Accuracy: 0.485800\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2017-06-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 20480) with dropout\n",
      "(?, 20480) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 20480) with dropout\n",
      "(?, 20480) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (3, 3)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*20, 'fc2' : 1024*20, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-16 22:54:38 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 1482635.7500 Trainning Accuracy: 0.225000 Validation Accuracy: 0.343200\n",
      "2017-06-17 01:26:35 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 1423748.7500 Trainning Accuracy: 0.350000 Validation Accuracy: 0.358000\n",
      "2017-06-17 01:42:39 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 1424686.0000 Trainning Accuracy: 0.375000 Validation Accuracy: 0.303800\n",
      "2017-06-17 01:58:11 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 684561.6875 Trainning Accuracy: 0.475000 Validation Accuracy: 0.400800\n",
      "2017-06-17 02:13:26 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 654955.0000 Trainning Accuracy: 0.350000 Validation Accuracy: 0.366800\n",
      "2017-06-17 02:28:36 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 333591.5312 Trainning Accuracy: 0.500000 Validation Accuracy: 0.434800\n",
      "2017-06-17 02:43:31 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 350210.0000 Trainning Accuracy: 0.475000 Validation Accuracy: 0.388200\n",
      "2017-06-17 02:58:26 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 354235.4688 Trainning Accuracy: 0.525000 Validation Accuracy: 0.369600\n",
      "2017-06-17 03:13:20 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 188600.9688 Trainning Accuracy: 0.600000 Validation Accuracy: 0.434600\n",
      "2017-06-17 03:28:14 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 126405.2109 Trainning Accuracy: 0.550000 Validation Accuracy: 0.426000\n",
      "2017-06-17 03:43:15 - Epoch 11, CIFAR-10 Batch 1:  Training Loss: 111225.1016 Trainning Accuracy: 0.500000 Validation Accuracy: 0.417600\n",
      "2017-06-17 03:58:33 - Epoch 12, CIFAR-10 Batch 1:  Training Loss: 100039.6016 Trainning Accuracy: 0.525000 Validation Accuracy: 0.425400\n",
      "2017-06-17 04:13:58 - Epoch 13, CIFAR-10 Batch 1:  Training Loss: 90348.6094 Trainning Accuracy: 0.575000 Validation Accuracy: 0.441000\n",
      "2017-06-17 04:29:26 - Epoch 14, CIFAR-10 Batch 1:  Training Loss: 51832.6484 Trainning Accuracy: 0.675000 Validation Accuracy: 0.436800\n",
      "2017-06-17 04:44:53 - Epoch 15, CIFAR-10 Batch 1:  Training Loss: 65771.8906 Trainning Accuracy: 0.500000 Validation Accuracy: 0.411000\n",
      "2017-06-17 05:00:23 - Epoch 16, CIFAR-10 Batch 1:  Training Loss: 34769.7578 Trainning Accuracy: 0.625000 Validation Accuracy: 0.457000\n",
      "2017-06-17 05:15:54 - Epoch 17, CIFAR-10 Batch 1:  Training Loss: 38351.8203 Trainning Accuracy: 0.550000 Validation Accuracy: 0.419200\n",
      "2017-06-17 05:31:28 - Epoch 18, CIFAR-10 Batch 1:  Training Loss: 34708.9062 Trainning Accuracy: 0.525000 Validation Accuracy: 0.404800\n",
      "2017-06-17 05:47:05 - Epoch 19, CIFAR-10 Batch 1:  Training Loss: 21297.9648 Trainning Accuracy: 0.550000 Validation Accuracy: 0.447400\n",
      "2017-06-17 06:02:43 - Epoch 20, CIFAR-10 Batch 1:  Training Loss: 23786.9004 Trainning Accuracy: 0.575000 Validation Accuracy: 0.414600\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 20\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-17 06:18:04 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 2315839.0000 Trainning Accuracy: 0.325000 Validation Accuracy: 0.223800\n",
      "2017-06-17 06:32:58 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 1733330.7500 Trainning Accuracy: 0.275000 Validation Accuracy: 0.263800\n",
      "2017-06-17 06:47:53 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 1057633.0000 Trainning Accuracy: 0.350000 Validation Accuracy: 0.309200\n",
      "2017-06-17 07:02:46 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 961327.6250 Trainning Accuracy: 0.350000 Validation Accuracy: 0.320200\n",
      "2017-06-17 07:17:40 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 457788.5000 Trainning Accuracy: 0.450000 Validation Accuracy: 0.392200\n",
      "2017-06-17 07:32:34 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 297844.3125 Trainning Accuracy: 0.400000 Validation Accuracy: 0.392800\n",
      "2017-06-17 07:47:29 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 182208.5156 Trainning Accuracy: 0.575000 Validation Accuracy: 0.434400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8b7c6cdb7f4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-81050f085b79>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# DONE: Implement Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \"\"\"\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .6 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testando o tamanho da convolução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 30)\n",
      "(?, 8, 8, 90)\n",
      "(?, 4, 4, 150)\n",
      "(?, 2400)\n",
      "(?, 10240) with dropout\n",
      "(?, 10240) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 30)\n",
      "(?, 8, 8, 90)\n",
      "(?, 4, 4, 150)\n",
      "(?, 2400)\n",
      "(?, 10240) with dropout\n",
      "(?, 10240) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 30, 'cvmp2': 90, 'cvmp3': 150}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (5, 5)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*10, 'fc2' : 1024*10, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-17 08:02:53 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 587576.6250 Trainning Accuracy: 0.300000 Validation Accuracy: 0.289800\n",
      "2017-06-17 08:09:48 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 301257.0000 Trainning Accuracy: 0.350000 Validation Accuracy: 0.365400\n",
      "2017-06-17 08:16:39 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 240017.1250 Trainning Accuracy: 0.475000 Validation Accuracy: 0.384000\n",
      "2017-06-17 08:23:30 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 153889.1250 Trainning Accuracy: 0.425000 Validation Accuracy: 0.396800\n",
      "2017-06-17 08:30:20 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 126153.1250 Trainning Accuracy: 0.450000 Validation Accuracy: 0.386600\n",
      "2017-06-17 08:37:09 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 62384.4688 Trainning Accuracy: 0.525000 Validation Accuracy: 0.395000\n",
      "2017-06-17 08:44:01 - Epoch  7, CIFAR-10 Batch 1:  Training Loss: 58233.7734 Trainning Accuracy: 0.575000 Validation Accuracy: 0.409800\n",
      "2017-06-17 08:50:54 - Epoch  8, CIFAR-10 Batch 1:  Training Loss: 38910.7930 Trainning Accuracy: 0.525000 Validation Accuracy: 0.433000\n",
      "2017-06-17 08:57:47 - Epoch  9, CIFAR-10 Batch 1:  Training Loss: 21373.4648 Trainning Accuracy: 0.675000 Validation Accuracy: 0.453800\n",
      "2017-06-17 09:04:40 - Epoch 10, CIFAR-10 Batch 1:  Training Loss: 26116.2090 Trainning Accuracy: 0.475000 Validation Accuracy: 0.433200\n"
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 33)\n",
      "(?, 8, 8, 99)\n",
      "(?, 4, 4, 165)\n",
      "(?, 2640)\n",
      "(?, 10240) with dropout\n",
      "(?, 10240) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 33)\n",
      "(?, 8, 8, 99)\n",
      "(?, 4, 4, 165)\n",
      "(?, 2640)\n",
      "(?, 10240) with dropout\n",
      "(?, 10240) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 3*11, 'cvmp2': 9*11, 'cvmp3': 15*11}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (5, 5)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*10, 'fc2' : 1024*10, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-17 09:22:50 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 612588.6250 Trainning Accuracy: 0.225000 Validation Accuracy: 0.247800\n",
      "2017-06-17 09:30:21 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 353295.4375 Trainning Accuracy: 0.425000 Validation Accuracy: 0.333400\n",
      "2017-06-17 09:37:47 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 258649.4062 Trainning Accuracy: 0.400000 Validation Accuracy: 0.332600\n",
      "2017-06-17 09:45:27 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 147059.0000 Trainning Accuracy: 0.400000 Validation Accuracy: 0.367200\n",
      "2017-06-17 09:53:10 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 99678.2812 Trainning Accuracy: 0.450000 Validation Accuracy: 0.387400\n",
      "2017-06-17 10:00:51 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 95625.3750 Trainning Accuracy: 0.400000 Validation Accuracy: 0.374000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-82e1bf9eaf1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-81050f085b79>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# DONE: Implement Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \"\"\"\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 100)\n",
      "(?, 4, 4, 256)\n",
      "(?, 4096)\n",
      "(?, 10240) with dropout\n",
      "(?, 10240) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 100)\n",
      "(?, 4, 4, 256)\n",
      "(?, 4096)\n",
      "(?, 10240) with dropout\n",
      "(?, 10240) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 100, 'cvmp3': 256}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (5, 5)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*10, 'fc2' : 1024*10, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-17 10:35:20 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 621291.3750 Trainning Accuracy: 0.350000 Validation Accuracy: 0.354200\n",
      "2017-06-17 10:43:32 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 479523.0000 Trainning Accuracy: 0.450000 Validation Accuracy: 0.361200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-82e1bf9eaf1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-81050f085b79>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# DONE: Implement Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \"\"\"\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 4096) with dropout\n",
      "(?, 4096) with dropout\n",
      "(?, 10)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2048)\n",
      "(?, 4096) with dropout\n",
      "(?, 4096) with dropout\n",
      "(?, 10)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = {'cvmp1': 32, 'cvmp2': 64, 'cvmp3': 128}\n",
    "    conv_ksize = {'cvmp1': (5, 5), 'cvmp2': (5, 5), 'cvmp3': (5, 5)}\n",
    "    conv_strides = {'cvmp1': (1, 1), 'cvmp2': (1, 1), 'cvmp3': (1, 1)}\n",
    "    pool_ksize = {'cvmp1': (3, 3), 'cvmp2': (3, 3), 'cvmp3': (3, 3)}\n",
    "    pool_strides = {'cvmp1': (2, 2), 'cvmp2': (2, 2), 'cvmp3': (2, 2)}\n",
    "    \n",
    "    num_outputs = {'fc1' : 1024*4, 'fc2' : 1024*4, 'fc3' : 1000, 'out': 10}\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "\n",
    "    # Layer 1 - (32, 32, 3) to (16, 16, 32)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp1'], conv_ksize['cvmp1'], conv_strides['cvmp1'], pool_ksize['cvmp1'], pool_strides['cvmp1'])\n",
    "    print(x.get_shape())\n",
    "    #print(x.get_shape(), end='')\n",
    "    #print('with dropout')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # Layer 2 - (16, 16, 32) to (8, 8, 64)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp2'], conv_ksize['cvmp2'], conv_strides['cvmp2'], pool_ksize['cvmp2'], pool_strides['cvmp2'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # Layer 3 - (8, 8, 64) to (4, 4, 128)\n",
    "    x = conv2d_maxpool(x, conv_num_outputs['cvmp3'], conv_ksize['cvmp3'], conv_strides['cvmp3'], pool_ksize['cvmp3'], pool_strides['cvmp3'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    # Flatten - (8, 8, 64) to 4096\n",
    "    x = flatten(x)\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # Fully Connected Layer 1 - 4096 to 1024\n",
    "    x = fully_conn(x, num_outputs['fc1'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 2 - 1024 to 512\n",
    "    x = fully_conn(x, num_outputs['fc2'])\n",
    "    print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    print(' with dropout')\n",
    "    \n",
    "    # Fully Connected Layer 3 - 512 to 256\n",
    "    #x = fully_conn(x, num_outputs['fc3'])\n",
    "    #print(x.get_shape(), end='')\n",
    "    \n",
    "    # Additional: Drop Out\n",
    "    #x = tf.nn.dropout(x, keep_prob)\n",
    "    #print(' with dropout')\n",
    "        \n",
    "    # DONE: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    # Output Layer - 512 to 10\n",
    "    x = output(x, num_outputs['out'])\n",
    "    print(x.get_shape())\n",
    "    \n",
    "    # DONE: return output\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy{}\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "2017-06-17 11:01:04 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 168972.6719 Trainning Accuracy: 0.250000 Validation Accuracy: 0.307200\n",
      "2017-06-17 11:04:41 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 84846.8516 Trainning Accuracy: 0.250000 Validation Accuracy: 0.364800\n",
      "2017-06-17 11:08:18 - Epoch  3, CIFAR-10 Batch 1:  Training Loss: 53970.5938 Trainning Accuracy: 0.350000 Validation Accuracy: 0.367800\n",
      "2017-06-17 11:11:42 - Epoch  4, CIFAR-10 Batch 1:  Training Loss: 29630.4629 Trainning Accuracy: 0.350000 Validation Accuracy: 0.386000\n",
      "2017-06-17 11:15:06 - Epoch  5, CIFAR-10 Batch 1:  Training Loss: 21786.0820 Trainning Accuracy: 0.375000 Validation Accuracy: 0.372200\n",
      "2017-06-17 11:18:27 - Epoch  6, CIFAR-10 Batch 1:  Training Loss: 11053.4941 Trainning Accuracy: 0.600000 Validation Accuracy: 0.393000\n",
      "2017-06-17 11:21:49 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:  8400.2402 Trainning Accuracy: 0.525000 Validation Accuracy: 0.395400\n",
      "2017-06-17 11:25:11 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:  5820.2393 Trainning Accuracy: 0.450000 Validation Accuracy: 0.363200\n",
      "2017-06-17 11:28:31 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:  2200.4788 Trainning Accuracy: 0.400000 Validation Accuracy: 0.299000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-82e1bf9eaf1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_neural_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-81050f085b79>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[1;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# DONE: Implement Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \"\"\"\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\peter\\Anaconda3\\envs\\tflearn\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DONE: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 128 #2**16 #128\n",
    "keep_probability = .75 # 0.75\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# treinamento com os parametros anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "2017-06-15 23:39:16 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 15185.3281 Trainning Accuracy: 0.175000 Validation Accuracy: 0.304800\n",
      "2017-06-15 23:43:27 - Epoch  1, CIFAR-10 Batch 2:  Training Loss: 11746.3379 Trainning Accuracy: 0.175000 Validation Accuracy: 0.315800\n",
      "2017-06-15 23:47:28 - Epoch  1, CIFAR-10 Batch 3:  Training Loss:  9597.2988 Trainning Accuracy: 0.425000 Validation Accuracy: 0.365600\n",
      "2017-06-15 23:51:28 - Epoch  1, CIFAR-10 Batch 4:  Training Loss:  6087.6704 Trainning Accuracy: 0.425000 Validation Accuracy: 0.355800\n",
      "2017-06-15 23:55:30 - Epoch  1, CIFAR-10 Batch 5:  Training Loss:  6706.9219 Trainning Accuracy: 0.425000 Validation Accuracy: 0.401400\n",
      "2017-06-15 23:59:36 - Epoch  2, CIFAR-10 Batch 1:  Training Loss:  6648.1924 Trainning Accuracy: 0.425000 Validation Accuracy: 0.427000\n",
      "2017-06-16 00:03:45 - Epoch  2, CIFAR-10 Batch 2:  Training Loss:  7369.9453 Trainning Accuracy: 0.400000 Validation Accuracy: 0.421200\n",
      "2017-06-16 00:08:07 - Epoch  2, CIFAR-10 Batch 3:  Training Loss:  6750.8374 Trainning Accuracy: 0.425000 Validation Accuracy: 0.415000\n",
      "2017-06-16 00:12:11 - Epoch  2, CIFAR-10 Batch 4:  Training Loss:  3557.9075 Trainning Accuracy: 0.575000 Validation Accuracy: 0.414000\n",
      "2017-06-16 00:16:09 - Epoch  2, CIFAR-10 Batch 5:  Training Loss:  3578.3159 Trainning Accuracy: 0.450000 Validation Accuracy: 0.421200\n",
      "2017-06-16 00:20:09 - Epoch  3, CIFAR-10 Batch 1:  Training Loss:  3750.8423 Trainning Accuracy: 0.450000 Validation Accuracy: 0.481000\n",
      "2017-06-16 00:24:09 - Epoch  3, CIFAR-10 Batch 2:  Training Loss:  3613.6145 Trainning Accuracy: 0.450000 Validation Accuracy: 0.445000\n",
      "2017-06-16 00:28:12 - Epoch  3, CIFAR-10 Batch 3:  Training Loss:  3120.1809 Trainning Accuracy: 0.550000 Validation Accuracy: 0.456400\n",
      "2017-06-16 00:32:14 - Epoch  3, CIFAR-10 Batch 4:  Training Loss:  2500.3755 Trainning Accuracy: 0.550000 Validation Accuracy: 0.496000\n",
      "2017-06-16 00:36:17 - Epoch  3, CIFAR-10 Batch 5:  Training Loss:  2467.8621 Trainning Accuracy: 0.475000 Validation Accuracy: 0.483800\n",
      "2017-06-16 00:40:22 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  3150.1172 Trainning Accuracy: 0.425000 Validation Accuracy: 0.413400\n",
      "2017-06-16 00:44:26 - Epoch  4, CIFAR-10 Batch 2:  Training Loss:  3261.5356 Trainning Accuracy: 0.500000 Validation Accuracy: 0.459800\n",
      "2017-06-16 00:48:31 - Epoch  4, CIFAR-10 Batch 3:  Training Loss:  2305.7358 Trainning Accuracy: 0.550000 Validation Accuracy: 0.430000\n",
      "2017-06-16 00:52:36 - Epoch  4, CIFAR-10 Batch 4:  Training Loss:  1696.9723 Trainning Accuracy: 0.600000 Validation Accuracy: 0.442200\n",
      "2017-06-16 00:56:41 - Epoch  4, CIFAR-10 Batch 5:  Training Loss:  1120.8010 Trainning Accuracy: 0.525000 Validation Accuracy: 0.485600\n",
      "2017-06-16 01:00:46 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  1604.3595 Trainning Accuracy: 0.450000 Validation Accuracy: 0.444600\n",
      "2017-06-16 01:04:53 - Epoch  5, CIFAR-10 Batch 2:  Training Loss:   835.6920 Trainning Accuracy: 0.525000 Validation Accuracy: 0.467000\n",
      "2017-06-16 01:08:59 - Epoch  5, CIFAR-10 Batch 3:  Training Loss:   700.4603 Trainning Accuracy: 0.525000 Validation Accuracy: 0.484800\n",
      "2017-06-16 01:13:05 - Epoch  5, CIFAR-10 Batch 4:  Training Loss:   538.0981 Trainning Accuracy: 0.575000 Validation Accuracy: 0.469200\n",
      "2017-06-16 01:17:12 - Epoch  5, CIFAR-10 Batch 5:  Training Loss:   287.6955 Trainning Accuracy: 0.525000 Validation Accuracy: 0.478400\n",
      "2017-06-16 01:21:20 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:   319.9492 Trainning Accuracy: 0.550000 Validation Accuracy: 0.489400\n",
      "2017-06-16 01:25:28 - Epoch  6, CIFAR-10 Batch 2:  Training Loss:   152.6899 Trainning Accuracy: 0.650000 Validation Accuracy: 0.479000\n",
      "2017-06-16 01:29:37 - Epoch  6, CIFAR-10 Batch 3:  Training Loss:   143.8261 Trainning Accuracy: 0.450000 Validation Accuracy: 0.451200\n",
      "2017-06-16 01:33:49 - Epoch  6, CIFAR-10 Batch 4:  Training Loss:    79.7484 Trainning Accuracy: 0.425000 Validation Accuracy: 0.418800\n",
      "2017-06-16 01:38:04 - Epoch  6, CIFAR-10 Batch 5:  Training Loss:    73.8741 Trainning Accuracy: 0.400000 Validation Accuracy: 0.388000\n",
      "2017-06-16 01:42:19 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:    47.4349 Trainning Accuracy: 0.400000 Validation Accuracy: 0.372000\n",
      "2017-06-16 01:46:32 - Epoch  7, CIFAR-10 Batch 2:  Training Loss:    22.7028 Trainning Accuracy: 0.500000 Validation Accuracy: 0.320000\n",
      "2017-06-16 01:50:46 - Epoch  7, CIFAR-10 Batch 3:  Training Loss:    25.3703 Trainning Accuracy: 0.275000 Validation Accuracy: 0.308000\n",
      "2017-06-16 01:54:58 - Epoch  7, CIFAR-10 Batch 4:  Training Loss:    10.4605 Trainning Accuracy: 0.325000 Validation Accuracy: 0.276000\n",
      "2017-06-16 01:59:10 - Epoch  7, CIFAR-10 Batch 5:  Training Loss:     6.9968 Trainning Accuracy: 0.350000 Validation Accuracy: 0.245000\n",
      "2017-06-16 02:03:21 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:    11.3360 Trainning Accuracy: 0.300000 Validation Accuracy: 0.221200\n",
      "2017-06-16 02:07:32 - Epoch  8, CIFAR-10 Batch 2:  Training Loss:    10.8595 Trainning Accuracy: 0.250000 Validation Accuracy: 0.207400\n",
      "2017-06-16 02:11:43 - Epoch  8, CIFAR-10 Batch 3:  Training Loss:     6.5959 Trainning Accuracy: 0.200000 Validation Accuracy: 0.202800\n",
      "2017-06-16 02:15:53 - Epoch  8, CIFAR-10 Batch 4:  Training Loss:     3.7882 Trainning Accuracy: 0.200000 Validation Accuracy: 0.188400\n",
      "2017-06-16 02:20:06 - Epoch  8, CIFAR-10 Batch 5:  Training Loss:     4.5690 Trainning Accuracy: 0.175000 Validation Accuracy: 0.181800\n",
      "2017-06-16 02:24:17 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:     4.5695 Trainning Accuracy: 0.225000 Validation Accuracy: 0.180800\n",
      "2017-06-16 02:28:29 - Epoch  9, CIFAR-10 Batch 2:  Training Loss:     3.0416 Trainning Accuracy: 0.275000 Validation Accuracy: 0.175800\n",
      "2017-06-16 02:32:41 - Epoch  9, CIFAR-10 Batch 3:  Training Loss:     3.9189 Trainning Accuracy: 0.125000 Validation Accuracy: 0.170000\n",
      "2017-06-16 02:36:52 - Epoch  9, CIFAR-10 Batch 4:  Training Loss:     3.9649 Trainning Accuracy: 0.150000 Validation Accuracy: 0.163400\n",
      "2017-06-16 02:41:04 - Epoch  9, CIFAR-10 Batch 5:  Training Loss:     3.0067 Trainning Accuracy: 0.125000 Validation Accuracy: 0.155600\n",
      "2017-06-16 02:45:15 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:     4.3111 Trainning Accuracy: 0.200000 Validation Accuracy: 0.156400\n",
      "2017-06-16 02:49:27 - Epoch 10, CIFAR-10 Batch 2:  Training Loss:     3.1283 Trainning Accuracy: 0.150000 Validation Accuracy: 0.147600\n",
      "2017-06-16 02:53:38 - Epoch 10, CIFAR-10 Batch 3:  Training Loss:     3.5377 Trainning Accuracy: 0.075000 Validation Accuracy: 0.144200\n",
      "2017-06-16 02:57:49 - Epoch 10, CIFAR-10 Batch 4:  Training Loss:     3.1684 Trainning Accuracy: 0.125000 Validation Accuracy: 0.136200\n",
      "2017-06-16 03:02:01 - Epoch 10, CIFAR-10 Batch 5:  Training Loss:     3.9535 Trainning Accuracy: 0.100000 Validation Accuracy: 0.149600\n",
      "2017-06-16 03:06:13 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:     2.6622 Trainning Accuracy: 0.225000 Validation Accuracy: 0.139800\n",
      "2017-06-16 03:10:25 - Epoch 11, CIFAR-10 Batch 2:  Training Loss:     3.1392 Trainning Accuracy: 0.150000 Validation Accuracy: 0.134200\n",
      "2017-06-16 03:14:38 - Epoch 11, CIFAR-10 Batch 3:  Training Loss:     2.1602 Trainning Accuracy: 0.200000 Validation Accuracy: 0.136400\n",
      "2017-06-16 03:18:50 - Epoch 11, CIFAR-10 Batch 4:  Training Loss:     2.3639 Trainning Accuracy: 0.075000 Validation Accuracy: 0.129600\n",
      "2017-06-16 03:23:03 - Epoch 11, CIFAR-10 Batch 5:  Training Loss:     2.4265 Trainning Accuracy: 0.075000 Validation Accuracy: 0.135200\n",
      "2017-06-16 03:27:16 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:     2.3199 Trainning Accuracy: 0.225000 Validation Accuracy: 0.132600\n",
      "2017-06-16 03:31:29 - Epoch 12, CIFAR-10 Batch 2:  Training Loss:     2.6581 Trainning Accuracy: 0.100000 Validation Accuracy: 0.133800\n",
      "2017-06-16 03:35:42 - Epoch 12, CIFAR-10 Batch 3:  Training Loss:     2.7332 Trainning Accuracy: 0.125000 Validation Accuracy: 0.127200\n",
      "2017-06-16 03:39:54 - Epoch 12, CIFAR-10 Batch 4:  Training Loss:     2.2113 Trainning Accuracy: 0.100000 Validation Accuracy: 0.124400\n",
      "2017-06-16 03:44:06 - Epoch 12, CIFAR-10 Batch 5:  Training Loss:     2.3608 Trainning Accuracy: 0.075000 Validation Accuracy: 0.132200\n",
      "2017-06-16 03:48:19 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:     2.1575 Trainning Accuracy: 0.175000 Validation Accuracy: 0.130000\n",
      "2017-06-16 03:52:32 - Epoch 13, CIFAR-10 Batch 2:  Training Loss:     2.7901 Trainning Accuracy: 0.150000 Validation Accuracy: 0.124200\n",
      "2017-06-16 03:56:45 - Epoch 13, CIFAR-10 Batch 3:  Training Loss:     2.3517 Trainning Accuracy: 0.125000 Validation Accuracy: 0.123600\n",
      "2017-06-16 04:00:58 - Epoch 13, CIFAR-10 Batch 4:  Training Loss:     2.2442 Trainning Accuracy: 0.100000 Validation Accuracy: 0.124000\n",
      "2017-06-16 04:05:12 - Epoch 13, CIFAR-10 Batch 5:  Training Loss:     2.3345 Trainning Accuracy: 0.100000 Validation Accuracy: 0.125600\n",
      "2017-06-16 04:09:26 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:     2.3435 Trainning Accuracy: 0.150000 Validation Accuracy: 0.114600\n",
      "2017-06-16 04:13:39 - Epoch 14, CIFAR-10 Batch 2:  Training Loss:     3.1720 Trainning Accuracy: 0.075000 Validation Accuracy: 0.116200\n",
      "2017-06-16 04:17:56 - Epoch 14, CIFAR-10 Batch 3:  Training Loss:     2.3601 Trainning Accuracy: 0.050000 Validation Accuracy: 0.119400\n",
      "2017-06-16 04:22:10 - Epoch 14, CIFAR-10 Batch 4:  Training Loss:     2.2211 Trainning Accuracy: 0.175000 Validation Accuracy: 0.112800\n",
      "2017-06-16 04:26:25 - Epoch 14, CIFAR-10 Batch 5:  Training Loss:     2.3534 Trainning Accuracy: 0.100000 Validation Accuracy: 0.115400\n",
      "2017-06-16 04:30:42 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:     2.3935 Trainning Accuracy: 0.150000 Validation Accuracy: 0.113400\n",
      "2017-06-16 04:34:57 - Epoch 15, CIFAR-10 Batch 2:  Training Loss:     2.4277 Trainning Accuracy: 0.100000 Validation Accuracy: 0.114000\n",
      "2017-06-16 04:39:12 - Epoch 15, CIFAR-10 Batch 3:  Training Loss:     2.4966 Trainning Accuracy: 0.100000 Validation Accuracy: 0.114600\n",
      "2017-06-16 04:43:26 - Epoch 15, CIFAR-10 Batch 4:  Training Loss:     2.2483 Trainning Accuracy: 0.175000 Validation Accuracy: 0.116000\n",
      "2017-06-16 04:47:42 - Epoch 15, CIFAR-10 Batch 5:  Training Loss:     2.2620 Trainning Accuracy: 0.125000 Validation Accuracy: 0.113200\n",
      "2017-06-16 04:51:57 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:     2.2384 Trainning Accuracy: 0.150000 Validation Accuracy: 0.110400\n",
      "2017-06-16 04:56:12 - Epoch 16, CIFAR-10 Batch 2:  Training Loss:     2.4090 Trainning Accuracy: 0.075000 Validation Accuracy: 0.111200\n",
      "2017-06-16 05:00:29 - Epoch 16, CIFAR-10 Batch 3:  Training Loss:     2.2673 Trainning Accuracy: 0.050000 Validation Accuracy: 0.117800\n",
      "2017-06-16 05:04:44 - Epoch 16, CIFAR-10 Batch 4:  Training Loss:     2.2215 Trainning Accuracy: 0.175000 Validation Accuracy: 0.107800\n",
      "2017-06-16 05:09:02 - Epoch 16, CIFAR-10 Batch 5:  Training Loss:     2.3031 Trainning Accuracy: 0.100000 Validation Accuracy: 0.114200\n",
      "2017-06-16 05:13:19 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:     2.8816 Trainning Accuracy: 0.150000 Validation Accuracy: 0.106000\n",
      "2017-06-16 05:17:37 - Epoch 17, CIFAR-10 Batch 2:  Training Loss:     2.4777 Trainning Accuracy: 0.075000 Validation Accuracy: 0.108200\n",
      "2017-06-16 05:21:56 - Epoch 17, CIFAR-10 Batch 3:  Training Loss:     2.2930 Trainning Accuracy: 0.100000 Validation Accuracy: 0.114200\n",
      "2017-06-16 05:26:14 - Epoch 17, CIFAR-10 Batch 4:  Training Loss:     2.2768 Trainning Accuracy: 0.175000 Validation Accuracy: 0.105400\n",
      "2017-06-16 05:30:30 - Epoch 17, CIFAR-10 Batch 5:  Training Loss:     2.3025 Trainning Accuracy: 0.100000 Validation Accuracy: 0.111400\n",
      "2017-06-16 05:34:47 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:     2.1445 Trainning Accuracy: 0.175000 Validation Accuracy: 0.106200\n",
      "2017-06-16 05:39:02 - Epoch 18, CIFAR-10 Batch 2:  Training Loss:     2.5459 Trainning Accuracy: 0.075000 Validation Accuracy: 0.109600\n",
      "2017-06-16 05:43:18 - Epoch 18, CIFAR-10 Batch 3:  Training Loss:     2.2869 Trainning Accuracy: 0.100000 Validation Accuracy: 0.118600\n",
      "2017-06-16 05:47:34 - Epoch 18, CIFAR-10 Batch 4:  Training Loss:     2.2452 Trainning Accuracy: 0.125000 Validation Accuracy: 0.110400\n",
      "2017-06-16 05:51:51 - Epoch 18, CIFAR-10 Batch 5:  Training Loss:     2.2452 Trainning Accuracy: 0.125000 Validation Accuracy: 0.115400\n",
      "2017-06-16 05:56:08 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     2.1895 Trainning Accuracy: 0.150000 Validation Accuracy: 0.102200\n",
      "2017-06-16 06:00:24 - Epoch 19, CIFAR-10 Batch 2:  Training Loss:     2.2091 Trainning Accuracy: 0.100000 Validation Accuracy: 0.103200\n",
      "2017-06-16 06:04:40 - Epoch 19, CIFAR-10 Batch 3:  Training Loss:     3.2437 Trainning Accuracy: 0.100000 Validation Accuracy: 0.115400\n",
      "2017-06-16 06:08:57 - Epoch 19, CIFAR-10 Batch 4:  Training Loss:     2.3098 Trainning Accuracy: 0.175000 Validation Accuracy: 0.100200\n",
      "2017-06-16 06:13:14 - Epoch 19, CIFAR-10 Batch 5:  Training Loss:     2.3021 Trainning Accuracy: 0.100000 Validation Accuracy: 0.107600\n",
      "2017-06-16 06:17:32 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:     2.9375 Trainning Accuracy: 0.100000 Validation Accuracy: 0.108600\n",
      "2017-06-16 06:21:50 - Epoch 20, CIFAR-10 Batch 2:  Training Loss:     2.2452 Trainning Accuracy: 0.075000 Validation Accuracy: 0.101800\n",
      "2017-06-16 06:26:09 - Epoch 20, CIFAR-10 Batch 3:  Training Loss:     2.4029 Trainning Accuracy: 0.125000 Validation Accuracy: 0.114200\n",
      "2017-06-16 06:30:27 - Epoch 20, CIFAR-10 Batch 4:  Training Loss:     2.2446 Trainning Accuracy: 0.125000 Validation Accuracy: 0.106000\n",
      "2017-06-16 06:34:45 - Epoch 20, CIFAR-10 Batch 5:  Training Loss:     2.3016 Trainning Accuracy: 0.125000 Validation Accuracy: 0.108600\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import datetime\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "2017-06-16 09:38:02 - Epoch  1, CIFAR-10 Batch 1:  Training Loss: 15688.2109 Trainning Accuracy: 0.375000 Validation Accuracy: 0.342400\n",
      "2017-06-16 09:42:19 - Epoch  1, CIFAR-10 Batch 2:  Training Loss: 12883.2891 Trainning Accuracy: 0.425000 Validation Accuracy: 0.368600\n",
      "2017-06-16 09:46:36 - Epoch  1, CIFAR-10 Batch 3:  Training Loss: 10200.7871 Trainning Accuracy: 0.450000 Validation Accuracy: 0.348000\n",
      "2017-06-16 09:50:54 - Epoch  1, CIFAR-10 Batch 4:  Training Loss:  9090.0898 Trainning Accuracy: 0.400000 Validation Accuracy: 0.435800\n",
      "2017-06-16 09:55:11 - Epoch  1, CIFAR-10 Batch 5:  Training Loss:  8943.7598 Trainning Accuracy: 0.500000 Validation Accuracy: 0.438800\n",
      "2017-06-16 09:59:30 - Epoch  2, CIFAR-10 Batch 1:  Training Loss: 11017.1436 Trainning Accuracy: 0.450000 Validation Accuracy: 0.362600\n",
      "2017-06-16 10:03:47 - Epoch  2, CIFAR-10 Batch 2:  Training Loss:  6188.4229 Trainning Accuracy: 0.550000 Validation Accuracy: 0.427200\n",
      "2017-06-16 10:08:07 - Epoch  2, CIFAR-10 Batch 3:  Training Loss:  4968.0234 Trainning Accuracy: 0.550000 Validation Accuracy: 0.458200\n",
      "2017-06-16 10:12:24 - Epoch  2, CIFAR-10 Batch 4:  Training Loss:  6805.8579 Trainning Accuracy: 0.275000 Validation Accuracy: 0.452000\n",
      "2017-06-16 10:16:41 - Epoch  2, CIFAR-10 Batch 5:  Training Loss:  5473.5845 Trainning Accuracy: 0.425000 Validation Accuracy: 0.466400\n",
      "2017-06-16 10:21:01 - Epoch  3, CIFAR-10 Batch 1:  Training Loss:  6382.2959 Trainning Accuracy: 0.475000 Validation Accuracy: 0.421200\n",
      "2017-06-16 10:25:24 - Epoch  3, CIFAR-10 Batch 2:  Training Loss:  4901.1504 Trainning Accuracy: 0.600000 Validation Accuracy: 0.458000\n",
      "2017-06-16 10:29:48 - Epoch  3, CIFAR-10 Batch 3:  Training Loss:  3693.9578 Trainning Accuracy: 0.625000 Validation Accuracy: 0.494200\n",
      "2017-06-16 10:34:11 - Epoch  3, CIFAR-10 Batch 4:  Training Loss:  4528.8896 Trainning Accuracy: 0.500000 Validation Accuracy: 0.474800\n",
      "2017-06-16 10:38:35 - Epoch  3, CIFAR-10 Batch 5:  Training Loss:  5543.5859 Trainning Accuracy: 0.400000 Validation Accuracy: 0.459200\n",
      "2017-06-16 10:43:00 - Epoch  4, CIFAR-10 Batch 1:  Training Loss:  4848.4409 Trainning Accuracy: 0.500000 Validation Accuracy: 0.486200\n",
      "2017-06-16 10:47:26 - Epoch  4, CIFAR-10 Batch 2:  Training Loss:  3169.2703 Trainning Accuracy: 0.575000 Validation Accuracy: 0.470800\n",
      "2017-06-16 10:51:53 - Epoch  4, CIFAR-10 Batch 3:  Training Loss:  2245.6255 Trainning Accuracy: 0.575000 Validation Accuracy: 0.474800\n",
      "2017-06-16 10:56:20 - Epoch  4, CIFAR-10 Batch 4:  Training Loss:  3904.1636 Trainning Accuracy: 0.525000 Validation Accuracy: 0.466200\n",
      "2017-06-16 11:00:47 - Epoch  4, CIFAR-10 Batch 5:  Training Loss:  2718.8030 Trainning Accuracy: 0.500000 Validation Accuracy: 0.501600\n",
      "2017-06-16 11:05:15 - Epoch  5, CIFAR-10 Batch 1:  Training Loss:  3710.5781 Trainning Accuracy: 0.575000 Validation Accuracy: 0.494800\n",
      "2017-06-16 11:09:44 - Epoch  5, CIFAR-10 Batch 2:  Training Loss:  1869.4186 Trainning Accuracy: 0.575000 Validation Accuracy: 0.508800\n",
      "2017-06-16 11:14:14 - Epoch  5, CIFAR-10 Batch 3:  Training Loss:  1567.0734 Trainning Accuracy: 0.600000 Validation Accuracy: 0.453000\n",
      "2017-06-16 11:18:44 - Epoch  5, CIFAR-10 Batch 4:  Training Loss:  1777.3893 Trainning Accuracy: 0.425000 Validation Accuracy: 0.513200\n",
      "2017-06-16 11:23:13 - Epoch  5, CIFAR-10 Batch 5:  Training Loss:  3035.1421 Trainning Accuracy: 0.375000 Validation Accuracy: 0.393600\n",
      "2017-06-16 11:27:45 - Epoch  6, CIFAR-10 Batch 1:  Training Loss:  1611.6713 Trainning Accuracy: 0.550000 Validation Accuracy: 0.506800\n",
      "2017-06-16 11:32:16 - Epoch  6, CIFAR-10 Batch 2:  Training Loss:  1235.2357 Trainning Accuracy: 0.525000 Validation Accuracy: 0.511800\n",
      "2017-06-16 11:36:48 - Epoch  6, CIFAR-10 Batch 3:  Training Loss:  1080.2180 Trainning Accuracy: 0.650000 Validation Accuracy: 0.504000\n",
      "2017-06-16 11:41:20 - Epoch  6, CIFAR-10 Batch 4:  Training Loss:   851.9661 Trainning Accuracy: 0.625000 Validation Accuracy: 0.517000\n",
      "2017-06-16 11:45:54 - Epoch  6, CIFAR-10 Batch 5:  Training Loss:   831.1193 Trainning Accuracy: 0.525000 Validation Accuracy: 0.479800\n",
      "2017-06-16 11:50:28 - Epoch  7, CIFAR-10 Batch 1:  Training Loss:   878.5936 Trainning Accuracy: 0.575000 Validation Accuracy: 0.495000\n",
      "2017-06-16 11:55:01 - Epoch  7, CIFAR-10 Batch 2:  Training Loss:   583.7036 Trainning Accuracy: 0.550000 Validation Accuracy: 0.491600\n",
      "2017-06-16 11:59:36 - Epoch  7, CIFAR-10 Batch 3:  Training Loss:   293.7996 Trainning Accuracy: 0.625000 Validation Accuracy: 0.491800\n",
      "2017-06-16 12:04:11 - Epoch  7, CIFAR-10 Batch 4:  Training Loss:   187.4570 Trainning Accuracy: 0.575000 Validation Accuracy: 0.468000\n",
      "2017-06-16 12:08:48 - Epoch  7, CIFAR-10 Batch 5:  Training Loss:   199.1829 Trainning Accuracy: 0.600000 Validation Accuracy: 0.472600\n",
      "2017-06-16 12:13:25 - Epoch  8, CIFAR-10 Batch 1:  Training Loss:   184.7657 Trainning Accuracy: 0.450000 Validation Accuracy: 0.469800\n",
      "2017-06-16 12:18:04 - Epoch  8, CIFAR-10 Batch 2:  Training Loss:   119.8060 Trainning Accuracy: 0.425000 Validation Accuracy: 0.456800\n",
      "2017-06-16 12:22:46 - Epoch  8, CIFAR-10 Batch 3:  Training Loss:    94.1902 Trainning Accuracy: 0.500000 Validation Accuracy: 0.436600\n",
      "2017-06-16 12:27:29 - Epoch  8, CIFAR-10 Batch 4:  Training Loss:    35.9002 Trainning Accuracy: 0.450000 Validation Accuracy: 0.398400\n",
      "2017-06-16 12:32:12 - Epoch  8, CIFAR-10 Batch 5:  Training Loss:    28.6305 Trainning Accuracy: 0.450000 Validation Accuracy: 0.367400\n",
      "2017-06-16 12:36:55 - Epoch  9, CIFAR-10 Batch 1:  Training Loss:    27.7952 Trainning Accuracy: 0.425000 Validation Accuracy: 0.348200\n",
      "2017-06-16 12:41:37 - Epoch  9, CIFAR-10 Batch 2:  Training Loss:    22.8519 Trainning Accuracy: 0.500000 Validation Accuracy: 0.315600\n",
      "2017-06-16 12:46:19 - Epoch  9, CIFAR-10 Batch 3:  Training Loss:    40.8858 Trainning Accuracy: 0.375000 Validation Accuracy: 0.301600\n",
      "2017-06-16 12:50:59 - Epoch  9, CIFAR-10 Batch 4:  Training Loss:    17.5229 Trainning Accuracy: 0.200000 Validation Accuracy: 0.254600\n",
      "2017-06-16 12:55:38 - Epoch  9, CIFAR-10 Batch 5:  Training Loss:    17.2111 Trainning Accuracy: 0.125000 Validation Accuracy: 0.231800\n",
      "2017-06-16 13:00:18 - Epoch 10, CIFAR-10 Batch 1:  Training Loss:    13.4068 Trainning Accuracy: 0.200000 Validation Accuracy: 0.225200\n",
      "2017-06-16 13:05:01 - Epoch 10, CIFAR-10 Batch 2:  Training Loss:    13.3184 Trainning Accuracy: 0.225000 Validation Accuracy: 0.217200\n",
      "2017-06-16 13:09:43 - Epoch 10, CIFAR-10 Batch 3:  Training Loss:    14.5823 Trainning Accuracy: 0.175000 Validation Accuracy: 0.208200\n",
      "2017-06-16 13:14:24 - Epoch 10, CIFAR-10 Batch 4:  Training Loss:     8.0031 Trainning Accuracy: 0.100000 Validation Accuracy: 0.194000\n",
      "2017-06-16 13:19:08 - Epoch 10, CIFAR-10 Batch 5:  Training Loss:     7.0818 Trainning Accuracy: 0.125000 Validation Accuracy: 0.172200\n",
      "2017-06-16 13:23:48 - Epoch 11, CIFAR-10 Batch 1:  Training Loss:     6.0670 Trainning Accuracy: 0.200000 Validation Accuracy: 0.181000\n",
      "2017-06-16 13:28:29 - Epoch 11, CIFAR-10 Batch 2:  Training Loss:     7.8382 Trainning Accuracy: 0.125000 Validation Accuracy: 0.164600\n",
      "2017-06-16 13:33:11 - Epoch 11, CIFAR-10 Batch 3:  Training Loss:     7.4436 Trainning Accuracy: 0.125000 Validation Accuracy: 0.166800\n",
      "2017-06-16 13:37:51 - Epoch 11, CIFAR-10 Batch 4:  Training Loss:     3.8290 Trainning Accuracy: 0.075000 Validation Accuracy: 0.153600\n",
      "2017-06-16 13:42:31 - Epoch 11, CIFAR-10 Batch 5:  Training Loss:     5.8195 Trainning Accuracy: 0.100000 Validation Accuracy: 0.147800\n",
      "2017-06-16 13:47:12 - Epoch 12, CIFAR-10 Batch 1:  Training Loss:     4.9633 Trainning Accuracy: 0.150000 Validation Accuracy: 0.149800\n",
      "2017-06-16 13:51:53 - Epoch 12, CIFAR-10 Batch 2:  Training Loss:     5.0519 Trainning Accuracy: 0.125000 Validation Accuracy: 0.155400\n",
      "2017-06-16 13:56:34 - Epoch 12, CIFAR-10 Batch 3:  Training Loss:     5.1402 Trainning Accuracy: 0.175000 Validation Accuracy: 0.145000\n",
      "2017-06-16 14:01:15 - Epoch 12, CIFAR-10 Batch 4:  Training Loss:     3.6467 Trainning Accuracy: 0.075000 Validation Accuracy: 0.139200\n",
      "2017-06-16 14:05:56 - Epoch 12, CIFAR-10 Batch 5:  Training Loss:     3.9534 Trainning Accuracy: 0.100000 Validation Accuracy: 0.144600\n",
      "2017-06-16 14:10:37 - Epoch 13, CIFAR-10 Batch 1:  Training Loss:     3.5813 Trainning Accuracy: 0.125000 Validation Accuracy: 0.145600\n",
      "2017-06-16 14:15:18 - Epoch 13, CIFAR-10 Batch 2:  Training Loss:     2.7653 Trainning Accuracy: 0.125000 Validation Accuracy: 0.143200\n",
      "2017-06-16 14:20:00 - Epoch 13, CIFAR-10 Batch 3:  Training Loss:     3.9886 Trainning Accuracy: 0.200000 Validation Accuracy: 0.133800\n",
      "2017-06-16 14:24:42 - Epoch 13, CIFAR-10 Batch 4:  Training Loss:     3.7925 Trainning Accuracy: 0.050000 Validation Accuracy: 0.131600\n",
      "2017-06-16 14:29:24 - Epoch 13, CIFAR-10 Batch 5:  Training Loss:     3.4131 Trainning Accuracy: 0.100000 Validation Accuracy: 0.137600\n",
      "2017-06-16 14:34:06 - Epoch 14, CIFAR-10 Batch 1:  Training Loss:     3.2076 Trainning Accuracy: 0.125000 Validation Accuracy: 0.132200\n",
      "2017-06-16 14:38:48 - Epoch 14, CIFAR-10 Batch 2:  Training Loss:     3.0831 Trainning Accuracy: 0.125000 Validation Accuracy: 0.133000\n",
      "2017-06-16 14:43:31 - Epoch 14, CIFAR-10 Batch 3:  Training Loss:     2.8644 Trainning Accuracy: 0.125000 Validation Accuracy: 0.131200\n",
      "2017-06-16 14:48:13 - Epoch 14, CIFAR-10 Batch 4:  Training Loss:     2.2581 Trainning Accuracy: 0.125000 Validation Accuracy: 0.132400\n",
      "2017-06-16 14:52:56 - Epoch 14, CIFAR-10 Batch 5:  Training Loss:     2.6780 Trainning Accuracy: 0.100000 Validation Accuracy: 0.133400\n",
      "2017-06-16 14:57:39 - Epoch 15, CIFAR-10 Batch 1:  Training Loss:     2.1640 Trainning Accuracy: 0.125000 Validation Accuracy: 0.128800\n",
      "2017-06-16 15:02:23 - Epoch 15, CIFAR-10 Batch 2:  Training Loss:     2.7593 Trainning Accuracy: 0.250000 Validation Accuracy: 0.136800\n",
      "2017-06-16 15:07:07 - Epoch 15, CIFAR-10 Batch 3:  Training Loss:     3.4646 Trainning Accuracy: 0.100000 Validation Accuracy: 0.141400\n",
      "2017-06-16 15:11:50 - Epoch 15, CIFAR-10 Batch 4:  Training Loss:     2.8857 Trainning Accuracy: 0.100000 Validation Accuracy: 0.131200\n",
      "2017-06-16 15:16:33 - Epoch 15, CIFAR-10 Batch 5:  Training Loss:     2.3595 Trainning Accuracy: 0.075000 Validation Accuracy: 0.134200\n",
      "2017-06-16 15:21:16 - Epoch 16, CIFAR-10 Batch 1:  Training Loss:     2.1684 Trainning Accuracy: 0.150000 Validation Accuracy: 0.137800\n",
      "2017-06-16 15:26:01 - Epoch 16, CIFAR-10 Batch 2:  Training Loss:     2.2381 Trainning Accuracy: 0.225000 Validation Accuracy: 0.146200\n",
      "2017-06-16 15:30:43 - Epoch 16, CIFAR-10 Batch 3:  Training Loss:     2.6014 Trainning Accuracy: 0.100000 Validation Accuracy: 0.143800\n",
      "2017-06-16 15:35:27 - Epoch 16, CIFAR-10 Batch 4:  Training Loss:     2.3078 Trainning Accuracy: 0.100000 Validation Accuracy: 0.132800\n",
      "2017-06-16 15:40:10 - Epoch 16, CIFAR-10 Batch 5:  Training Loss:     2.2170 Trainning Accuracy: 0.125000 Validation Accuracy: 0.132400\n",
      "2017-06-16 15:44:55 - Epoch 17, CIFAR-10 Batch 1:  Training Loss:     2.1896 Trainning Accuracy: 0.150000 Validation Accuracy: 0.138400\n",
      "2017-06-16 15:49:40 - Epoch 17, CIFAR-10 Batch 2:  Training Loss:     2.5514 Trainning Accuracy: 0.150000 Validation Accuracy: 0.122600\n",
      "2017-06-16 15:54:38 - Epoch 17, CIFAR-10 Batch 3:  Training Loss:     4.0952 Trainning Accuracy: 0.125000 Validation Accuracy: 0.124400\n",
      "2017-06-16 15:59:28 - Epoch 17, CIFAR-10 Batch 4:  Training Loss:     2.4137 Trainning Accuracy: 0.150000 Validation Accuracy: 0.118400\n",
      "2017-06-16 16:04:15 - Epoch 17, CIFAR-10 Batch 5:  Training Loss:     2.1835 Trainning Accuracy: 0.125000 Validation Accuracy: 0.124400\n",
      "2017-06-16 16:09:00 - Epoch 18, CIFAR-10 Batch 1:  Training Loss:     2.1769 Trainning Accuracy: 0.150000 Validation Accuracy: 0.124400\n",
      "2017-06-16 16:13:45 - Epoch 18, CIFAR-10 Batch 2:  Training Loss:     2.6083 Trainning Accuracy: 0.150000 Validation Accuracy: 0.121800\n",
      "2017-06-16 16:18:30 - Epoch 18, CIFAR-10 Batch 3:  Training Loss:     2.2572 Trainning Accuracy: 0.175000 Validation Accuracy: 0.119800\n",
      "2017-06-16 16:23:17 - Epoch 18, CIFAR-10 Batch 4:  Training Loss:     2.3154 Trainning Accuracy: 0.100000 Validation Accuracy: 0.126000\n",
      "2017-06-16 16:28:02 - Epoch 18, CIFAR-10 Batch 5:  Training Loss:     2.6017 Trainning Accuracy: 0.100000 Validation Accuracy: 0.114400\n",
      "2017-06-16 16:32:48 - Epoch 19, CIFAR-10 Batch 1:  Training Loss:     2.1773 Trainning Accuracy: 0.150000 Validation Accuracy: 0.131400\n",
      "2017-06-16 16:37:34 - Epoch 19, CIFAR-10 Batch 2:  Training Loss:     2.1914 Trainning Accuracy: 0.100000 Validation Accuracy: 0.113600\n",
      "2017-06-16 16:42:21 - Epoch 19, CIFAR-10 Batch 3:  Training Loss:     2.8211 Trainning Accuracy: 0.075000 Validation Accuracy: 0.144000\n",
      "2017-06-16 16:47:08 - Epoch 19, CIFAR-10 Batch 4:  Training Loss:     2.2466 Trainning Accuracy: 0.125000 Validation Accuracy: 0.115800\n",
      "2017-06-16 16:51:58 - Epoch 19, CIFAR-10 Batch 5:  Training Loss:     2.4854 Trainning Accuracy: 0.150000 Validation Accuracy: 0.124000\n",
      "2017-06-16 16:56:47 - Epoch 20, CIFAR-10 Batch 1:  Training Loss:     2.6652 Trainning Accuracy: 0.100000 Validation Accuracy: 0.123800\n",
      "2017-06-16 17:01:36 - Epoch 20, CIFAR-10 Batch 2:  Training Loss:     2.1274 Trainning Accuracy: 0.125000 Validation Accuracy: 0.114800\n",
      "2017-06-16 17:06:24 - Epoch 20, CIFAR-10 Batch 3:  Training Loss:     2.3272 Trainning Accuracy: 0.075000 Validation Accuracy: 0.119600\n",
      "2017-06-16 17:11:12 - Epoch 20, CIFAR-10 Batch 4:  Training Loss:     2.3025 Trainning Accuracy: 0.100000 Validation Accuracy: 0.111200\n",
      "2017-06-16 17:15:59 - Epoch 20, CIFAR-10 Batch 5:  Training Loss:     2.3752 Trainning Accuracy: 0.125000 Validation Accuracy: 0.117000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import datetime\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('{:%Y-%m-%d %H:%M:%S} - Epoch {:>2}, CIFAR-10 Batch {}:  '.format(datetime.datetime.now(),epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
